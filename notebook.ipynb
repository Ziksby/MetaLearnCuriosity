{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from:\n",
    "# https://github.com/corl-team/xland-minigrid/blob/main/training/train_single_task.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "import optax\n",
    "import wandb\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "from flax.training.train_state import TrainState\n",
    "import numpy as np\n",
    "from MetaLearnCuriosity.agents.nn import MiniGridActorCriticRNN, MiniGridBYOLPredictor, BYOLTarget,OpenScannedRNN,CloseScannedRNN\n",
    "from MetaLearnCuriosity.checkpoints import Save\n",
    "from MetaLearnCuriosity.logger import WBLogger\n",
    "from MetaLearnCuriosity.utils import BYOLMiniGridTransition as Transition\n",
    "from MetaLearnCuriosity.utils import (\n",
    "    byol_calculate_gae,\n",
    "    byol_minigrid_ppo_update_networks,\n",
    "    rnn_rollout,\n",
    "    BYOLRewardNorm\n",
    ")\n",
    "from MetaLearnCuriosity.wrappers import (\n",
    "    FlattenObservationWrapper,\n",
    "    LogWrapper,\n",
    "    MiniGridGymnax,\n",
    "    VecEnv,\n",
    ")\n",
    "\n",
    "jax.config.update(\"jax_threefry_partitionable\", True)\n",
    "\n",
    "environments = [\n",
    "    # \"MiniGrid-BlockedUnlockPickUp\",\n",
    "    # \"MiniGrid-DoorKey-16x16\",\n",
    "    \"MiniGrid-Empty-16x16\",\n",
    "    # \"MiniGrid-EmptyRandom-16x16\",\n",
    "    # \"MiniGrid-FourRooms\",\n",
    "    # \"MiniGrid-LockedRoom\",\n",
    "    # \"MiniGrid-MemoryS128\",\n",
    "    # \"MiniGrid-Unlock\",\n",
    "    # \"MiniGrid-UnlockPickUp\",\n",
    "]\n",
    "\n",
    "config = {\n",
    "    \"NUM_SEEDS\": 1,\n",
    "    \"PROJECT\": \"MetaLearnCuriosity\",\n",
    "    \"RUN_NAME\": \"minigrid-ppo-baseline\",\n",
    "    \"BENCHMARK_ID\": None,\n",
    "    \"RULESET_ID\": None,\n",
    "    \"USE_CNNS\": False,\n",
    "    # Agent\n",
    "    \"ACTION_EMB_DIM\": 16,\n",
    "    \"RNN_HIDDEN_DIM\": 1024,\n",
    "    \"RNN_NUM_LAYERS\": 1,\n",
    "    \"HEAD_HIDDEN_DIM\": 256,\n",
    "    # Training\n",
    "    \"NUM_ENVS\": 8192,\n",
    "    \"NUM_STEPS\": 16,\n",
    "    \"UPDATE_EPOCHS\": 1,\n",
    "    \"NUM_MINIBATCHES\": 16,\n",
    "    \"TOTAL_TIMESTEPS\": 50_000_000,\n",
    "    \"LR\": 0.001,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"EVAL_EPISODES\": 80,\n",
    "    \"SEED\": 42,\n",
    "    \"ANNEAL_PRED_LR\": False,\n",
    "    \"DEBUG\": False,\n",
    "    \"PRED_LR\": 0.001,\n",
    "    \"REW_NORM_PARAMETER\": 0.99,\n",
    "    \"EMA_PARAMETER\": 0.99,\n",
    "}\n",
    "\n",
    "\n",
    "def make_env_config(config, env_name):\n",
    "    num_devices = jax.local_device_count()\n",
    "    config[\"ENV_NAME\"] = env_name\n",
    "    assert config[\"NUM_ENVS\"] % num_devices == 0\n",
    "    env = MiniGridGymnax(config[\"ENV_NAME\"])\n",
    "    env_params = env._env_params\n",
    "    env_eval = MiniGridGymnax(config[\"ENV_NAME\"])\n",
    "    if config[\"USE_CNNS\"]:\n",
    "        observations_shape = env.observation_space(env_params).shape[0]\n",
    "    else:\n",
    "        env = FlattenObservationWrapper(env)\n",
    "        observations_shape = env.observation_space(env_params).shape\n",
    "    env_eval = LogWrapper(env_eval)\n",
    "    env = LogWrapper(env)\n",
    "    num_devices = jax.local_device_count()\n",
    "    config[\"NUM_ENVS_PER_DEVICE\"] = config[\"NUM_ENVS\"] // num_devices\n",
    "    config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] = config[\"TOTAL_TIMESTEPS\"] // num_devices\n",
    "    config[\"EVAL_EPISODES_PER_DEVICE\"] = config[\"EVAL_EPISODES\"] // num_devices\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "    )\n",
    "    print(f\"Num devices: {num_devices}, Num updates: {config['NUM_UPDATES']}\")\n",
    "    return observations_shape, config, env, env_params\n",
    "\n",
    "\n",
    "def make_train(rng):\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    rng, _tar_rng = jax.random.split(rng)\n",
    "    # rng, _en_rng = jax.random.split(rng)\n",
    "    rng, _pred_rng = jax.random.split(rng)\n",
    "    num_actions = env.action_space(env_params).n\n",
    "    def pred_linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"PRED_LR\"] * frac\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "        # INIT network\n",
    "    target = BYOLTarget(256)\n",
    "    pred = MiniGridBYOLPredictor(256, num_actions)\n",
    "    network = MiniGridActorCriticRNN(\n",
    "        num_actions=num_actions,\n",
    "        action_emb_dim=config[\"ACTION_EMB_DIM\"],\n",
    "        rnn_hidden_dim=config[\"RNN_HIDDEN_DIM\"],\n",
    "        rnn_num_layers=config[\"RNN_NUM_LAYERS\"],\n",
    "        head_hidden_dim=config[\"HEAD_HIDDEN_DIM\"],\n",
    "        use_cnns=config[\"USE_CNNS\"],\n",
    "    )\n",
    "    init_obs = {\n",
    "        \"observation\": jnp.zeros((config[\"NUM_ENVS_PER_DEVICE\"], 1, *observations_shape)),\n",
    "        \"prev_action\": jnp.zeros((config[\"NUM_ENVS_PER_DEVICE\"], 1), dtype=jnp.int32),\n",
    "        \"prev_reward\": jnp.zeros((config[\"NUM_ENVS_PER_DEVICE\"], 1)),\n",
    "    }\n",
    "    init_hstate = network.initialize_carry(batch_size=config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "    init_x = jnp.zeros((config[\"NUM_ENVS_PER_DEVICE\"], 1, *observations_shape))\n",
    "    init_action = jnp.zeros((config[\"NUM_ENVS_PER_DEVICE\"],1), dtype=jnp.int32)\n",
    "    close_init_hstate = pred.initialize_carry(config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "    open_init_hstate = pred.initialize_carry(config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "    print(init_hstate.shape, close_init_hstate.shape,open_init_hstate.shape)\n",
    "    init_bt = jnp.zeros((config[\"NUM_ENVS_PER_DEVICE\"],1, 256))\n",
    "    init_pred_input = (init_bt, init_x, init_action)\n",
    "    pred_params = pred.init(_pred_rng, close_init_hstate, open_init_hstate, init_pred_input)\n",
    "    target_params = target.init(_tar_rng, init_x)\n",
    "\n",
    "    network_params = network.init(_rng, init_obs, init_hstate)\n",
    "    pred_params = pred.init(_pred_rng, close_init_hstate, open_init_hstate, init_pred_input)\n",
    "    target_params = target.init(_tar_rng, init_x)\n",
    "\n",
    "    tx = optax.chain(\n",
    "        optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "        optax.inject_hyperparams(optax.adam)(learning_rate=linear_schedule, eps=1e-8),  # eps=1e-5\n",
    "    )\n",
    "    if config[\"ANNEAL_PRED_LR\"]:\n",
    "        pred_tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(learning_rate=pred_linear_schedule, eps=1e-5),\n",
    "        )\n",
    "    else:\n",
    "        pred_tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(config[\"PRED_LR\"], eps=1e-5),\n",
    "        )\n",
    "    pred_state = TrainState.create(\n",
    "        apply_fn=pred.apply,\n",
    "        params=pred_params,\n",
    "        tx=pred_tx,\n",
    "    )\n",
    "\n",
    "    target_state = TrainState.create(\n",
    "        apply_fn=target.apply,\n",
    "        params=target_params,\n",
    "        tx=pred_tx,\n",
    "    )\n",
    "    train_state = TrainState.create(apply_fn=network.apply, params=network_params, tx=tx)\n",
    "    # env = VecEnv(env)\n",
    "    rng = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "    return init_hstate,close_init_hstate,open_init_hstate, train_state,pred_state,target_state,rng\n",
    "\n",
    "\n",
    "def train(rng, init_hstate,close_init_hstate,open_init_hstate, train_state,pred_state,target_state):\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "\n",
    "    reset_rng = jax.random.split(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "    # INIT STUFF FOR OPTIMIZATION AND NORMALIZATION\n",
    "    update_target_counter = 0\n",
    "    byol_reward_norm_params = BYOLRewardNorm(0, 0, 1, 0)\n",
    "    \n",
    "    obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)\n",
    "    prev_action = jnp.zeros(config[\"NUM_ENVS_PER_DEVICE\"], dtype=jnp.int32)\n",
    "    prev_reward = jnp.zeros(config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "    prev_bt = jnp.zeros((config[\"NUM_ENVS_PER_DEVICE\"], 1,256))\n",
    "\n",
    "    # TRAIN LOOP\n",
    "\n",
    "    def _update_step(runner_state, _):\n",
    "\n",
    "        # COLLECT TRAJECTORIES\n",
    "\n",
    "        def _env_step(runner_state, _):\n",
    "            (\n",
    "                rng,\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                env_state,\n",
    "                prev_obs,\n",
    "                prev_action,\n",
    "                prev_reward,\n",
    "                prev_bt,\n",
    "                prev_hstate,\n",
    "                close_prev_hstate,\n",
    "                open_prev_hstate,\n",
    "                byol_reward_norm_params,\n",
    "                update_target_counter,\n",
    "            ) = runner_state\n",
    "\n",
    "            # SELECT ACTION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            dist, value, hstate = train_state.apply_fn(\n",
    "                train_state.params,\n",
    "                {\n",
    "                    # [batch_size, seq_len=1, ...]\n",
    "                    \"observation\": prev_obs[:, None],\n",
    "                    \"prev_action\": prev_action[:, None],\n",
    "                    \"prev_reward\": prev_reward[:, None],\n",
    "                },\n",
    "                prev_hstate,\n",
    "            )\n",
    "            action, log_prob = dist.sample_and_log_prob(seed=_rng)\n",
    "            # squeeze seq_len where possible\n",
    "            action, value, log_prob = action.squeeze(1), value.squeeze(1), log_prob.squeeze(1)\n",
    "\n",
    "            # STEP ENV\n",
    "            rng_step = jax.random.split(rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "            obsv, env_state, reward, done, info = jax.vmap(env.step, in_axes=(0, 0, 0, None))(\n",
    "                rng_step, env_state, action, env_params\n",
    "            )\n",
    "            # INT REWARD\n",
    "            tar_obs = target_state.apply_fn(target_state.params, obsv[:,None])\n",
    "            pred_input = (prev_bt, prev_obs[:,None], prev_action[:,None])\n",
    "            pred_obs, new_bt, new_close_hstate, new_open_hstate = pred_state.apply_fn(\n",
    "                pred_state.params, close_prev_hstate, open_prev_hstate, pred_input\n",
    "            )\n",
    "            pred_norm = (pred_obs.squeeze(1)) / (\n",
    "                jnp.linalg.norm(pred_obs.squeeze(1), axis=-1, keepdims=True)\n",
    "            )\n",
    "            tar_norm = jax.lax.stop_gradient(\n",
    "                (tar_obs.squeeze(1)) / (jnp.linalg.norm(tar_obs.squeeze(1), axis=-1, keepdims=True))\n",
    "            )\n",
    "\n",
    "            int_reward = jnp.square(jnp.linalg.norm((pred_norm - tar_norm), axis=-1)) * (1 - done)\n",
    "            transition = Transition(\n",
    "                done=done,\n",
    "                action=action,\n",
    "                value=value,\n",
    "                reward=reward,\n",
    "                int_reward=int_reward,\n",
    "                log_prob=log_prob,\n",
    "                obs=prev_obs,\n",
    "                next_obs=obsv,\n",
    "                prev_action=prev_action,\n",
    "                prev_reward=prev_reward,\n",
    "                prev_bt=prev_bt,\n",
    "                info=info,\n",
    "            )\n",
    "            runner_state = (rng, train_state, pred_state,target_state,env_state, obsv, action, reward, new_bt,hstate,new_close_hstate,new_open_hstate,byol_reward_norm_params,update_target_counter)\n",
    "            return runner_state, transition\n",
    "\n",
    "        initial_hstate,close_initial_hstate,open_initial_hstate = runner_state[9:12]\n",
    "        runner_state, transitions = jax.lax.scan(_env_step, runner_state, None, config[\"NUM_STEPS\"])\n",
    "\n",
    "        # CALCULATE ADVANTAGE\n",
    "\n",
    "        rng, train_state,pred_state,target_state, env_state, prev_obs, prev_action, prev_reward,prev_bt, hstate,close_hstate,open_hstate,byol_reward_norm_params,update_target_counter = runner_state\n",
    "\n",
    "        _, last_val, _ = train_state.apply_fn(\n",
    "            train_state.params,\n",
    "            {\n",
    "                \"observation\": prev_obs[:, None],\n",
    "                \"prev_action\": prev_action[:, None],\n",
    "                \"prev_reward\": prev_reward[:, None],\n",
    "            },\n",
    "            hstate,\n",
    "        )\n",
    "\n",
    "        advantages, targets,norm_int_reward, byol_reward_norm_params = byol_calculate_gae(\n",
    "            transitions, last_val.squeeze(1), config[\"GAMMA\"], config[\"GAE_LAMBDA\"],config[\"INT_LAMBDA\"],config[\"REW_NORM_PARAMETER\"],byol_reward_norm_params\n",
    "        )\n",
    "\n",
    "        # UPDATE NETWORK\n",
    "        def _update_epoch(update_state, _):\n",
    "            def _update_minbatch(train_states, batch_info):\n",
    "                init_hstate, close_init_hstate,open_init_hstate,transitions, advantages, targets = batch_info\n",
    "                train_state,pred_state,target_state,update_target_counter = train_states\n",
    "                (new_train_state,pred_state,target_state,update_target_counter) ,update_info = byol_minigrid_ppo_update_networks(\n",
    "                    train_state=train_state,\n",
    "                    pred_state=pred_state,\n",
    "                    target_state=target_state,\n",
    "                    transitions=transitions,\n",
    "                    init_hstate=init_hstate.squeeze(1),\n",
    "                    init_close_hstate=close_init_hstate.squeeze(1),\n",
    "                    init_open_hstate=open_init_hstate.squeeze(1),\n",
    "                    advantages=advantages,\n",
    "                    targets=targets,\n",
    "                    clip_eps=config[\"CLIP_EPS\"],\n",
    "                    vf_coef=config[\"VF_COEF\"],\n",
    "                    ent_coef=config[\"ENT_COEF\"],\n",
    "                    update_target_counter=update_target_counter,\n",
    "                    ema_param=config[\"EMA_PARAMETER\"],\n",
    "                )\n",
    "                return (new_train_state,pred_state,target_state,update_target_counter), update_info\n",
    "\n",
    "            rng, train_state,pred_state,target_state,update_target_counter, init_hstate,close_init_hstate,open_init_hstate, transitions, advantages, targets = update_state\n",
    "\n",
    "            # MINIBATCHES PREPARATION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            permutation = jax.random.permutation(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "            # [seq_len, batch_size, ...]\n",
    "            batch = (init_hstate,close_init_hstate,open_init_hstate, transitions, advantages, targets)\n",
    "            # [batch_size, seq_len, ...], as our model assumes\n",
    "            batch = jtu.tree_map(lambda x: x.swapaxes(0, 1), batch)\n",
    "\n",
    "            shuffled_batch = jtu.tree_map(lambda x: jnp.take(x, permutation, axis=0), batch)\n",
    "            # [num_minibatches, minibatch_size, ...]\n",
    "            minibatches = jtu.tree_map(\n",
    "                lambda x: jnp.reshape(x, (config[\"NUM_MINIBATCHES\"], -1) + x.shape[1:]),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "            (train_state,pred_state,target_state,update_target_counter), update_info = jax.lax.scan(_update_minbatch, (train_state,pred_state,target_state,update_target_counter), minibatches)\n",
    "\n",
    "            update_state = (rng, train_state,pred_state,target_state,update_target_counter, init_hstate,close_init_hstate,open_init_hstate, transitions, advantages, targets)\n",
    "            return update_state, update_info\n",
    "\n",
    "        # [seq_len, batch_size, num_layers, hidden_dim]\n",
    "        init_hstate = initial_hstate[None, :]\n",
    "        close_init_hstate = close_initial_hstate[None,:]\n",
    "        open_init_hstate = open_initial_hstate[None,:]\n",
    "        update_state = (rng, train_state,pred_state,target_state,update_target_counter, init_hstate,close_init_hstate,open_init_hstate, transitions, advantages, targets)\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "        )\n",
    "\n",
    "        # averaging over minibatches then over epochs\n",
    "        # loss_info = jtu.tree_map(lambda x: x.mean(-1).mean(-1), loss_info)\n",
    "\n",
    "        # EVALUATE AGENT\n",
    "        # rng, _rng = jax.random.split(rng)\n",
    "        # eval_rng = jax.random.split(_rng, num=config[\"EVAL_EPISODES_PER_DEVICE\"])\n",
    "\n",
    "        # # vmap only on rngs\n",
    "        # eval_stats = jax.vmap(rnn_rollout, in_axes=(0, None, None, None, None, None))(\n",
    "        #     eval_rng,\n",
    "        #     env_eval,\n",
    "        #     env_params,\n",
    "        #     train_state,\n",
    "        #     # TODO: make this as a static method mb?\n",
    "        #     jnp.zeros((1, config[\"RNN_NUM_LAYERS\"], config[\"RNN_HIDDEN_DIM\"])),\n",
    "        #     1,\n",
    "        # )\n",
    "        # eval_stats = jax.lax.pmean(eval_stats, axis_name=\"devices\")\n",
    "        # loss_info.update(\n",
    "        #     {\n",
    "        #         \"eval/returns\": eval_stats.reward.mean(0),\n",
    "        #         \"eval/lengths\": eval_stats.length.mean(0),\n",
    "        #         \"lr\": train_state.opt_state[-1].hyperparams[\"learning_rate\"],\n",
    "        #     }\n",
    "        # )\n",
    "\n",
    "        rng, train_state,pred_state,target_state,update_target_counter = update_state[:5]\n",
    "        traj_batch = update_state[-3]\n",
    "        metric = traj_batch.info\n",
    "        int_reward=traj_batch.int_reward\n",
    "        runner_state = (rng, train_state, pred_state,target_state,env_state, prev_obs, prev_action, prev_reward,prev_bt, hstate,close_hstate,open_hstate, byol_reward_norm_params,update_target_counter)\n",
    "        return runner_state, (metric, loss_info,int_reward,norm_int_reward)\n",
    "\n",
    "    runner_state = (rng, train_state, pred_state,target_state,env_state, obsv, prev_action, prev_reward, prev_bt, init_hstate, close_init_hstate,open_init_hstate,byol_reward_norm_params,update_target_counter)\n",
    "    runner_state, (metric,loss,int_reward,norm_int_reward) = jax.lax.scan(_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
    "  \n",
    "    return {\n",
    "        \"runner_state\": runner_state,\n",
    "        \"metrics\": metric,\n",
    "        \"loss_info\": loss,\n",
    "        \"rl_total_loss\": loss[\"total_loss\"],\n",
    "        \"rl_value_loss\": loss[\"value_loss\"],\n",
    "        \"rl_actor_loss\": loss[\"actor_loss\"],\n",
    "        \"rl_entrophy_loss\": loss[\"entropy\"],\n",
    "        \"int_reward\": int_reward,\n",
    "        \"norm_int_reward\": norm_int_reward,\n",
    "        \"pred_loss\":loss[\"pred_loss\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num devices: 4, Num updates: 381\n",
      "(2048, 1, 1024) (2048, 1, 256) (2048, 1, 256)\n"
     ]
    }
   ],
   "source": [
    "# lambda_values = jnp.array(\n",
    "#     [0.001, 0.0001, 0.0003, 0.0005, 0.0008, 0.01, 0.1, 0.003, 0.005, 0.02, 0.03, 0.05]\n",
    "# ).sort()\n",
    "lambda_values = jnp.array(\n",
    "    [0.001]\n",
    ").sort()\n",
    "y_values = {}\n",
    "for lambda_value in lambda_values:\n",
    "    y_values[\n",
    "        float(lambda_value)\n",
    "    ] = {}  # Use float(lambda_value) to ensure dictionary keys are serializable\n",
    "    config[\"INT_LAMBDA\"] = lambda_value\n",
    "    for env_name in environments:\n",
    "\n",
    "        observations_shape, config, env, env_params = make_env_config(config, env_name)\n",
    "\n",
    "        # experiments\n",
    "        rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "\n",
    "        if config[\"NUM_SEEDS\"] > 1:\n",
    "            rng = jax.random.split(rng, config[\"NUM_SEEDS\"])\n",
    "            init_hstate,close_init_hstate,open_init_hstate, train_state,pred_state,target_state,rng= jax.jit(jax.vmap(make_train, out_axes=(0, 0, 0,0,0,0,1)))(rng)\n",
    "            open_init_hstate = replicate(open_init_hstate, jax.local_devices())\n",
    "            close_init_hstate = replicate(close_init_hstate, jax.local_devices())\n",
    "            train_state = replicate(train_state, jax.local_devices())\n",
    "            pred_state = replicate(pred_state, jax.local_devices())\n",
    "            target_state = replicate(target_state, jax.local_devices())\n",
    "\n",
    "            train_fn = jax.vmap(train)\n",
    "            train_fn = jax.pmap(train_fn, axis_name=\"devices\")\n",
    "            print(f\"Training in {config['ENV_NAME']}\")\n",
    "            t = time.time()\n",
    "            output = jax.block_until_ready(train_fn(rng, init_hstate,close_init_hstate,open_init_hstate, train_state, pred_state, target_state))\n",
    "            elapsed_time = time.time() - t\n",
    "            epi_ret = (\n",
    "                output[\"metrics\"][\"returned_episode_returns\"].mean(0).mean(0).mean(-1).reshape(-1)\n",
    "            )\n",
    "            int_rew = output[\"int_reward\"].mean(0).mean(0).mean(-1).reshape(-1)\n",
    "            int_norm_rew = output[\"norm_int_reward\"].mean(0).mean(0).mean(-1).reshape(-1)\n",
    "            pred_loss = unreplicate(output[\"pred_loss\"]).mean(-1).mean(0).mean(-1)\n",
    "            # output = unreplicate(output)\n",
    "\n",
    "        else:\n",
    "            init_hstate,close_init_hstate,open_init_hstate, train_state,pred_state,target_state,rng = make_train(rng)\n",
    "            init_hstate = replicate(init_hstate, jax.local_devices())\n",
    "            open_init_hstate = replicate(open_init_hstate, jax.local_devices())\n",
    "            close_init_hstate = replicate(close_init_hstate, jax.local_devices())\n",
    "            train_state = replicate(train_state, jax.local_devices())\n",
    "            pred_state = replicate(pred_state, jax.local_devices())\n",
    "            target_state = replicate(target_state, jax.local_devices())\n",
    "            train_fn = jax.pmap(train, axis_name=\"devices\")\n",
    "            t = time.time()\n",
    "            output = jax.block_until_ready(train_fn(rng, init_hstate,close_init_hstate,open_init_hstate, train_state, pred_state, target_state))\n",
    "            # output = unreplicate(output)\n",
    "            epi_ret = output[\"metrics\"][\"returned_episode_returns\"].mean(0).mean(-1).reshape(-1)\n",
    "            int_rew = output[\"int_reward\"].mean(0).mean(-1).reshape(-1)\n",
    "            int_norm_rew = output[\"norm_int_reward\"].mean(0).mean(-1).reshape(-1)\n",
    "            pred_loss = unreplicate(output[\"pred_loss\"]).mean(-1).mean(-1)\n",
    "\n",
    "        print((time.time() - t) / 60)\n",
    "        # Assuming `output` is your array\n",
    "\n",
    "        # Use the last element of each row from 'epi_ret' as y-values\n",
    "        y_values[float(lambda_value)][env_name] = (epi_ret, int_rew, int_norm_rew, pred_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'MiniGrid-Empty-16x16'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Collect data for plotting\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lambda_value, env_data \u001b[38;5;129;01min\u001b[39;00m y_values\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 55\u001b[0m     epi_ret \u001b[38;5;241m=\u001b[39m \u001b[43menv_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# index 0 for episode returns\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epi_ret\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Ensure there is at least one return value\u001b[39;00m\n\u001b[1;32m     57\u001b[0m         final_returns\u001b[38;5;241m.\u001b[39mappend(epi_ret[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mKeyError\u001b[0m: 'MiniGrid-Empty-16x16'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metric names corresponding to the data stored in y_values\n",
    "metric_names = [\n",
    "    \"Episode Returns\",\n",
    "    \"Intrinsic Reward\",\n",
    "    \"Normalized Intrinsic Reward\",\n",
    "    \"Pred Loss\",\n",
    "]\n",
    "\n",
    "# Initialize plotting\n",
    "for env_name in environments:\n",
    "    num_metrics = len(metric_names)\n",
    "    fig, axs = plt.subplots(num_metrics, 1, figsize=(12, 6 * num_metrics), sharex=False)\n",
    "    fig.suptitle(f\"Training Metrics Over Time for {env_name}\")\n",
    "\n",
    "    # Iterate over each metric\n",
    "    for idx, metric_name in enumerate(metric_names):\n",
    "        ax = axs[idx] if num_metrics > 1 else axs\n",
    "\n",
    "        # Plot each lambda's data for this metric\n",
    "        plotted = False\n",
    "        for lambda_value in lambda_values:\n",
    "            lambda_key = float(lambda_value)  # Ensure float key matches dictionary keys\n",
    "            if lambda_key in y_values and env_name in y_values[lambda_key]:\n",
    "                metric_data = y_values[lambda_key][env_name][idx]\n",
    "                if len(metric_data) > 0:\n",
    "                    x_axis = range(1, len(metric_data) + 1)\n",
    "                    ax.plot(x_axis, metric_data, label=f\"Lambda={lambda_value:.5f}\")\n",
    "                    plotted = True\n",
    "\n",
    "        # Only add a legend if data was actually plotted\n",
    "        if plotted:\n",
    "            ax.set_title(metric_name)\n",
    "            ax.set_xlabel(\"Training Steps\")\n",
    "            ax.set_ylabel(metric_name)\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.set_title(f\"{metric_name} (no data)\")\n",
    "            ax.set_xlabel(\"Training Steps\")\n",
    "            ax.set_ylabel(metric_name)\n",
    "\n",
    "    # Adjust layout and save the figure\n",
    "    plt.subplots_adjust(hspace=0.4)  # Adjust vertical spacing between plots\n",
    "    plt.savefig(f\"{env_name}_metrics_over_time_sep_byol_TEST.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# Scatter plot for final episode returns vs. lambda values\n",
    "for env_name in environments:\n",
    "    lambda_values = []\n",
    "    final_returns = []\n",
    "\n",
    "    # Collect data for plotting\n",
    "    for lambda_value, env_data in y_values.items():\n",
    "        epi_ret = env_data[env_name][0]  # index 0 for episode returns\n",
    "        if epi_ret.size > 0:  # Ensure there is at least one return value\n",
    "            final_returns.append(epi_ret[-1])\n",
    "            lambda_values.append(lambda_value)\n",
    "\n",
    "    # Sort lambda values for consistent plotting\n",
    "    sorted_indices = sorted(range(len(lambda_values)), key=lambda k: lambda_values[k])\n",
    "    sorted_lambda_values = [lambda_values[i] for i in sorted_indices]\n",
    "    sorted_final_returns = [final_returns[i] for i in sorted_indices]\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(range(1, len(sorted_lambda_values) + 1), sorted_final_returns, color=\"blue\")\n",
    "    plt.title(f\"Final Episode Returns vs. Lambda for {env_name}\")\n",
    "    plt.xlabel(\"Lambda Value (Indexed)\")\n",
    "    plt.ylabel(\"Final Episode Return\")\n",
    "    plt.xticks(\n",
    "        range(1, len(sorted_lambda_values) + 1),\n",
    "        [f\"{lv:.5f}\" for lv in sorted_lambda_values],\n",
    "        rotation=45,\n",
    "    )\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Save and close plot\n",
    "    plt.savefig(f\"{env_name}_final_episode_returns_vs_lambda_sep_byol_TEST.png\")\n",
    "    plt.close()  # Close the plot to free up memory\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpu_curiosax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
