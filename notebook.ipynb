{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batsy/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Taken from:\n",
    "# https://github.com/corl-team/xland-minigrid/blob/main/training/train_single_task.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from typing import Sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "import distrax\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import gymnax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util\n",
    "import numpy as np\n",
    "import optax\n",
    "import wandb\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from MetaLearnCuriosity.agents.nn import (\n",
    "    AtariBYOLPredictor,\n",
    "    BYOLTarget,\n",
    "    CloseScannedRNN,\n",
    "    OpenScannedRNN,\n",
    "    TemporalRewardCombiner\n",
    ")\n",
    "from MetaLearnCuriosity.pmapped_open_es import OpenES\n",
    "from MetaLearnCuriosity.checkpoints import Save\n",
    "from MetaLearnCuriosity.logger import WBLogger\n",
    "from MetaLearnCuriosity.utils import BYOLRewardNorm\n",
    "from MetaLearnCuriosity.utils import RCBYOLTransition as Transition\n",
    "from MetaLearnCuriosity.utils import (\n",
    "    byol_normalize_prior_int_rewards,\n",
    "    process_output_general,\n",
    "    update_target_state_with_ema,\n",
    ")\n",
    "from MetaLearnCuriosity.wrappers import FlattenObservationWrapper, LogWrapper, VecEnv\n",
    "\n",
    "environments = [\n",
    "    \"Asterix-MinAtar\",\n",
    "    # \"Breakout-MinAtar\",\n",
    "    # \"Freeway-MinAtar\",\n",
    "    # \"SpaceInvaders-MinAtar\",\n",
    "]\n",
    "\n",
    "\n",
    "class PPOActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
    "            actor_mean\n",
    "        )\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        critic = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(critic)\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"RUN_NAME\": \"minatar_byol_ppo\",\n",
    "    \"SEED\": 42,\n",
    "    \"NUM_SEEDS\": 8,\n",
    "    \"LR\": 5e-3,\n",
    "    \"NUM_ENVS\": 64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"ANNEAL_PRED_LR\": False,\n",
    "    \"DEBUG\": False,\n",
    "    \"PRED_LR\": 0.001,\n",
    "    \"REW_NORM_PARAMETER\": 0.99,\n",
    "    \"EMA_PARAMETER\": 0.99,\n",
    "    \"POP_SIZE\":8,\n",
    "    \"ES_SEED\": 7,\n",
    "    \"RC_SEED\": 23,\n",
    "    \"NUM_GENERATIONS\":2,\n",
    "    # \"INT_LAMBDA\": 0.001,\n",
    "}\n",
    "\n",
    "environments = [\n",
    "    \"Asterix-MinAtar\",\n",
    "    # \"Breakout-MinAtar\",\n",
    "    # \"Freeway-MinAtar\",\n",
    "    # \"SpaceInvaders-MinAtar\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_config_env(config, env_name):\n",
    "    config[\"ENV_NAME\"] = env_name\n",
    "    num_devices = jax.local_device_count()\n",
    "    assert config[\"NUM_ENVS\"] % num_devices == 0\n",
    "    config[\"NUM_ENVS_PER_DEVICE\"] = config[\"NUM_ENVS\"] // num_devices\n",
    "    config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] = config[\"TOTAL_TIMESTEPS\"] // num_devices\n",
    "    # config[\"EVAL_EPISODES_PER_DEVICE\"] = config[\"EVAL_EPISODES\"] // num_devices\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS_PER_DEVICE\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    config[\"TRAINING_HORIZON\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] // config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "    )\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "    env = VecEnv(env)\n",
    "\n",
    "    return config, env, env_params\n",
    "\n",
    "\n",
    "def ppo_make_train(rng):\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def pred_linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"PRED_LR\"] * frac\n",
    "\n",
    "    # INIT NETWORK\n",
    "    network = PPOActorCritic(env.action_space(env_params).n, activation=config[\"ACTIVATION\"])\n",
    "    target = BYOLTarget(128)\n",
    "    pred = AtariBYOLPredictor(128, env.action_space(env_params).n)\n",
    "\n",
    "    # KEYS\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    rng, _tar_rng = jax.random.split(rng)\n",
    "    # rng, _en_rng = jax.random.split(rng)\n",
    "    rng, _pred_rng = jax.random.split(rng)\n",
    "\n",
    "    # INIT INPUT\n",
    "    init_x = jnp.zeros((1, config[\"NUM_ENVS_PER_DEVICE\"], *env.observation_space(env_params).shape))\n",
    "    init_action = jnp.zeros((config[\"NUM_ENVS_PER_DEVICE\"],), dtype=jnp.int32)\n",
    "    close_init_hstate = CloseScannedRNN.initialize_carry(config[\"NUM_ENVS_PER_DEVICE\"], 128)\n",
    "    open_init_hstate = OpenScannedRNN.initialize_carry(config[\"NUM_ENVS_PER_DEVICE\"], 128)\n",
    "    init_bt = jnp.zeros((1, config[\"NUM_ENVS_PER_DEVICE\"], 128))\n",
    "\n",
    "    init_pred_input = (init_bt, init_x, init_action[np.newaxis, :])\n",
    "\n",
    "    network_params = network.init(_rng, init_x)\n",
    "    pred_params = pred.init(_pred_rng, close_init_hstate, open_init_hstate, init_pred_input)\n",
    "    target_params = target.init(_tar_rng, init_x)\n",
    "\n",
    "    if config[\"ANNEAL_LR\"]:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "        )\n",
    "    else:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(config[\"LR\"], eps=1e-5),\n",
    "        )\n",
    "\n",
    "    if config[\"ANNEAL_PRED_LR\"]:\n",
    "        pred_tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(learning_rate=pred_linear_schedule, eps=1e-5),\n",
    "        )\n",
    "    else:\n",
    "        pred_tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(config[\"PRED_LR\"], eps=1e-5),\n",
    "        )\n",
    "\n",
    "    train_state = TrainState.create(\n",
    "        apply_fn=network.apply,\n",
    "        params=network_params,\n",
    "        tx=tx,\n",
    "    )\n",
    "    pred_state = TrainState.create(\n",
    "        apply_fn=pred.apply,\n",
    "        params=pred_params,\n",
    "        tx=pred_tx,\n",
    "    )\n",
    "\n",
    "    target_state = TrainState.create(\n",
    "        apply_fn=target.apply,\n",
    "        params=target_params,\n",
    "        tx=pred_tx,\n",
    "    )\n",
    "\n",
    "    rng = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "    return (\n",
    "        rng,\n",
    "        train_state,\n",
    "        pred_state,\n",
    "        target_state,\n",
    "        init_bt,\n",
    "        close_init_hstate,\n",
    "        open_init_hstate,\n",
    "        init_action,\n",
    "    )\n",
    "\n",
    "\n",
    "def train(\n",
    "    rng,\n",
    "    rc_params,\n",
    "    train_state,\n",
    "    pred_state,\n",
    "    target_state,\n",
    "    init_bt,\n",
    "    close_init_hstate,\n",
    "    open_init_hstate,\n",
    "    init_action,\n",
    "):\n",
    "    # REWARD COMBINER\n",
    "    rc_network=TemporalRewardCombiner()\n",
    "    # INIT STUFF FOR OPTIMIZATION AND NORMALIZATION\n",
    "    update_target_counter = 0\n",
    "    byol_reward_norm_params = BYOLRewardNorm(0, 0, 1, 0)\n",
    "\n",
    "    # INIT ENV\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "    obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "    # TRAIN LOOP\n",
    "    def _update_step(runner_state, unused):\n",
    "        # COLLECT TRAJECTORIES\n",
    "        def _env_step(runner_state, unused):\n",
    "            (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                bt,\n",
    "                close_hstate,\n",
    "                open_hstate,\n",
    "                last_act,\n",
    "                env_state,\n",
    "                last_obs,\n",
    "                byol_reward_norm_params,\n",
    "                update_target_counter,\n",
    "                rng,\n",
    "            ) = runner_state\n",
    "\n",
    "            # SELECT ACTION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            pi, value = train_state.apply_fn(train_state.params, last_obs[np.newaxis, :])\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # STEP ENV\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            rng_step = jax.random.split(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "            obsv, env_state, reward, done, info = env.step(\n",
    "                rng_step, env_state, action.squeeze(0), env_params\n",
    "            )\n",
    "\n",
    "            # TIME STEP\n",
    "\n",
    "            norm_time_step = info[\"timestep\"]/config[\"TRAINING_HORIZON\"]\n",
    "\n",
    "            # INT REWARD\n",
    "            tar_obs = target_state.apply_fn(target_state.params, obsv[np.newaxis, :])\n",
    "            pred_input = (bt, last_obs[np.newaxis, :], last_act[np.newaxis, :])\n",
    "            pred_obs, new_bt, new_close_hstate, new_open_hstate = pred_state.apply_fn(\n",
    "                pred_state.params, close_hstate, open_hstate, pred_input\n",
    "            )\n",
    "            pred_norm = (pred_obs.squeeze(0)) / (\n",
    "                jnp.linalg.norm(pred_obs.squeeze(0), axis=-1, keepdims=True)\n",
    "            )\n",
    "            tar_norm = jax.lax.stop_gradient(\n",
    "                (tar_obs.squeeze(0)) / (jnp.linalg.norm(tar_obs.squeeze(0), axis=-1, keepdims=True))\n",
    "            )\n",
    "            int_reward = jnp.square(jnp.linalg.norm((pred_norm - tar_norm), axis=-1)) * (1 - done)\n",
    "            value, action, log_prob = (value.squeeze(0), action.squeeze(0), log_prob.squeeze(0))\n",
    "            transition = Transition(\n",
    "                done,\n",
    "                last_act,\n",
    "                action,\n",
    "                value,\n",
    "                reward,\n",
    "                int_reward,\n",
    "                log_prob,\n",
    "                last_obs,\n",
    "                obsv,\n",
    "                bt,\n",
    "                norm_time_step,\n",
    "                info,\n",
    "            )\n",
    "            runner_state = (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                new_bt,\n",
    "                new_close_hstate,\n",
    "                new_open_hstate,\n",
    "                action,\n",
    "                env_state,\n",
    "                obsv,\n",
    "                byol_reward_norm_params,\n",
    "                update_target_counter,\n",
    "                rng,\n",
    "            )\n",
    "            return runner_state, transition\n",
    "\n",
    "        close_initial_hstate, open_initial_hstate = runner_state[4:6]\n",
    "        runner_state, traj_batch = jax.lax.scan(_env_step, runner_state, None, config[\"NUM_STEPS\"])\n",
    "\n",
    "        # CALCULATE ADVANTAGE\n",
    "        (\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            target_state,\n",
    "            bt,\n",
    "            close_hstate,\n",
    "            open_hstate,\n",
    "            last_act,\n",
    "            env_state,\n",
    "            last_obs,\n",
    "            byol_reward_norm_params,\n",
    "            update_target_counter,\n",
    "            rng,\n",
    "        ) = runner_state\n",
    "\n",
    "        # update_target_counter+=1\n",
    "        _, last_val = train_state.apply_fn(train_state.params, last_obs[np.newaxis, :])\n",
    "\n",
    "        def _calculate_gae(traj_batch, last_val, byol_reward_norm_params):\n",
    "            norm_int_reward, byol_reward_norm_params = byol_normalize_prior_int_rewards(\n",
    "                traj_batch.int_reward, byol_reward_norm_params, config[\"REW_NORM_PARAMETER\"]\n",
    "            )\n",
    "            norm_traj_batch = Transition(\n",
    "                traj_batch.done,\n",
    "                traj_batch.prev_action,\n",
    "                traj_batch.action,\n",
    "                traj_batch.value,\n",
    "                traj_batch.reward,\n",
    "                norm_int_reward,\n",
    "                traj_batch.log_prob,\n",
    "                traj_batch.obs,\n",
    "                traj_batch.next_obs,\n",
    "                traj_batch.bt,\n",
    "                traj_batch.norm_time_step,\n",
    "                traj_batch.info,\n",
    "            )\n",
    "\n",
    "            def _get_advantages(gae_and_next_value, transition):\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward, int_reward, norm_time_step = (\n",
    "                    transition.done,\n",
    "                    transition.value,\n",
    "                    transition.reward,\n",
    "                    transition.int_reward,\n",
    "                    transition.norm_time_step,\n",
    "                )\n",
    "                rc_input = jnp.concatenate(\n",
    "                    (reward[:, None], int_reward[:, None], norm_time_step[:, None]), axis=-1\n",
    "                )              \n",
    "                int_lambda = rc_network.apply(rc_params,rc_input)\n",
    "                delta = (\n",
    "                    (reward + (int_reward * int_lambda))\n",
    "                    + config[\"GAMMA\"] * next_value * (1 - done)\n",
    "                    - value\n",
    "                )\n",
    "                gae = delta + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                norm_traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return (\n",
    "                advantages,\n",
    "                advantages + traj_batch.value,\n",
    "                norm_int_reward,\n",
    "                byol_reward_norm_params,\n",
    "            )\n",
    "\n",
    "        advantages, targets, norm_int_reward, byol_reward_norm_params = _calculate_gae(\n",
    "            traj_batch, last_val.squeeze(0), byol_reward_norm_params\n",
    "        )\n",
    "\n",
    "        # UPDATE NETWORK\n",
    "        def _update_epoch(update_state, unused):\n",
    "            def _update_minbatch(train_states, batch_info):\n",
    "                traj_batch, advantages, targets, init_close_hstate, init_open_hstate = batch_info\n",
    "                train_state, pred_state, target_state, update_target_counter = train_states\n",
    "\n",
    "                def pred_loss(\n",
    "                    pred_params, target_params, traj_batch, init_close_hstate, init_open_hstate\n",
    "                ):\n",
    "                    tar_obs = target_state.apply_fn(target_params, traj_batch.next_obs)\n",
    "                    pred_input = (traj_batch.bt, traj_batch.obs, traj_batch.prev_action)\n",
    "                    pred_obs, _, _, _ = pred_state.apply_fn(\n",
    "                        pred_params, init_close_hstate[0], init_open_hstate[0], pred_input\n",
    "                    )\n",
    "                    pred_norm = (pred_obs) / (jnp.linalg.norm(pred_obs, axis=-1, keepdims=True))\n",
    "                    tar_norm = jax.lax.stop_gradient(\n",
    "                        (tar_obs) / (jnp.linalg.norm(tar_obs, axis=-1, keepdims=True))\n",
    "                    )\n",
    "                    loss = jnp.square(jnp.linalg.norm((pred_norm - tar_norm), axis=-1)) * (\n",
    "                        1 - traj_batch.done\n",
    "                    )\n",
    "                    return loss.mean()\n",
    "\n",
    "                def _loss_fn(params, traj_batch, gae, targets):\n",
    "                    # RERUN NETWORK\n",
    "                    pi, value = train_state.apply_fn(params, traj_batch.obs)\n",
    "                    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                    # CALCULATE VALUE LOSS\n",
    "                    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "                        -config[\"CLIP_EPS\"], config[\"CLIP_EPS\"]\n",
    "                    )\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "                    # CALCULATE ACTOR LOSS\n",
    "                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = (\n",
    "                        jnp.clip(\n",
    "                            ratio,\n",
    "                            1.0 - config[\"CLIP_EPS\"],\n",
    "                            1.0 + config[\"CLIP_EPS\"],\n",
    "                        )\n",
    "                        * gae\n",
    "                    )\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                    loss_actor = loss_actor.mean()\n",
    "                    entropy = pi.entropy().mean()\n",
    "\n",
    "                    total_loss = (\n",
    "                        loss_actor + config[\"VF_COEF\"] * value_loss - config[\"ENT_COEF\"] * entropy\n",
    "                    )\n",
    "                    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                (loss, (vloss, aloss, entropy)), grads = jax.value_and_grad(_loss_fn, has_aux=True)(\n",
    "                    train_state.params, traj_batch, advantages, targets\n",
    "                )\n",
    "                pred_losses, pred_grads = jax.value_and_grad(pred_loss)(\n",
    "                    pred_state.params,\n",
    "                    target_state.params,\n",
    "                    traj_batch,\n",
    "                    init_close_hstate,\n",
    "                    init_open_hstate,\n",
    "                )\n",
    "                # (loss, vloss, aloss, entropy, pred_losses, grads, pred_grads) = jax.lax.pmean(\n",
    "                #     (loss, vloss, aloss, entropy, pred_losses, grads, pred_grads),\n",
    "                #     axis_name=\"devices\",\n",
    "                # )\n",
    "\n",
    "                def update_target(\n",
    "                    target_state, pred_state, update_target_counter=update_target_counter\n",
    "                ):\n",
    "                    def true_fun(_):\n",
    "                        # Perform the EMA update\n",
    "                        return update_target_state_with_ema(\n",
    "                            predictor_state=pred_state,\n",
    "                            target_state=target_state,\n",
    "                            ema_param=config[\"EMA_PARAMETER\"],\n",
    "                        )\n",
    "\n",
    "                    def false_fun(_):\n",
    "                        # Return the old target_params unchanged\n",
    "                        return target_state\n",
    "\n",
    "                    # Conditionally update every 10 steps\n",
    "                    return jax.lax.cond(\n",
    "                        update_target_counter % 320 == 0,\n",
    "                        true_fun,\n",
    "                        false_fun,\n",
    "                        None,  # The argument passed to true_fun and false_fun, `_` in this case is unused\n",
    "                    )\n",
    "\n",
    "                update_target_counter += 1\n",
    "                train_state = train_state.apply_gradients(grads=grads)\n",
    "                pred_state = pred_state.apply_gradients(grads=pred_grads)\n",
    "                target_state = update_target(target_state, pred_state, update_target_counter)\n",
    "\n",
    "                return (train_state, pred_state, target_state, update_target_counter), (\n",
    "                    loss,\n",
    "                    (vloss, aloss, entropy),\n",
    "                    pred_losses,\n",
    "                )\n",
    "\n",
    "            (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                update_target_counter,\n",
    "                init_close_hstate,\n",
    "                init_open_hstate,\n",
    "                traj_batch,\n",
    "                advantages,\n",
    "                targets,\n",
    "                rng,\n",
    "            ) = update_state\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            permutation = jax.random.permutation(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "            batch = (traj_batch, advantages, targets, init_close_hstate, init_open_hstate)\n",
    "\n",
    "            shuffled_batch = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=1), batch\n",
    "            )\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.swapaxes(\n",
    "                    jnp.reshape(\n",
    "                        x,\n",
    "                        [x.shape[0], config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[2:]),\n",
    "                    ),\n",
    "                    1,\n",
    "                    0,\n",
    "                ),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "            (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                update_target_counter,\n",
    "            ), total_loss = jax.lax.scan(\n",
    "                _update_minbatch,\n",
    "                (train_state, pred_state, target_state, update_target_counter),\n",
    "                minibatches,\n",
    "            )\n",
    "            update_state = (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                update_target_counter,\n",
    "                init_close_hstate,\n",
    "                init_open_hstate,\n",
    "                traj_batch,\n",
    "                advantages,\n",
    "                targets,\n",
    "                rng,\n",
    "            )\n",
    "            return update_state, total_loss\n",
    "\n",
    "        traj_batch = Transition(\n",
    "            traj_batch.done,\n",
    "            traj_batch.prev_action,\n",
    "            traj_batch.action,\n",
    "            traj_batch.value,\n",
    "            traj_batch.reward,\n",
    "            traj_batch.int_reward,\n",
    "            traj_batch.log_prob,\n",
    "            traj_batch.obs,\n",
    "            traj_batch.next_obs,\n",
    "            traj_batch.bt.squeeze(1),\n",
    "            traj_batch.norm_time_step,\n",
    "            traj_batch.info,\n",
    "        )\n",
    "\n",
    "        update_state = (\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            target_state,\n",
    "            update_target_counter,\n",
    "            open_initial_hstate[np.newaxis, :],\n",
    "            close_initial_hstate[np.newaxis, :],\n",
    "            traj_batch,\n",
    "            advantages,\n",
    "            targets,\n",
    "            rng,\n",
    "        )\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "        )\n",
    "        train_state, pred_state, target_state, update_target_counter = update_state[:4]\n",
    "        metric = traj_batch.info\n",
    "        rng = update_state[-1]\n",
    "        if config.get(\"DEBUG\"):\n",
    "\n",
    "            def callback(info):\n",
    "                return_values = info[\"returned_episode_returns\"][info[\"returned_episode\"]]\n",
    "                timesteps = (\n",
    "                    info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "                )\n",
    "                for t in range(len(timesteps)):\n",
    "                    print(f\"global step={timesteps[t]}, episodic return={return_values[t]}\")\n",
    "\n",
    "            jax.debug.callback(callback, metric)\n",
    "\n",
    "        runner_state = (\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            target_state,\n",
    "            bt,\n",
    "            close_hstate,\n",
    "            open_hstate,\n",
    "            last_act,\n",
    "            env_state,\n",
    "            last_obs,\n",
    "            byol_reward_norm_params,\n",
    "            update_target_counter,\n",
    "            rng,\n",
    "        )\n",
    "        return runner_state, (metric, loss_info, norm_int_reward, traj_batch.int_reward)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (\n",
    "        train_state,\n",
    "        pred_state,\n",
    "        target_state,\n",
    "        init_bt,\n",
    "        close_init_hstate,\n",
    "        open_init_hstate,\n",
    "        init_action,\n",
    "        env_state,\n",
    "        obsv,\n",
    "        byol_reward_norm_params,\n",
    "        update_target_counter,\n",
    "        _rng,\n",
    "    )\n",
    "    runner_state, extra_info = jax.lax.scan(_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
    "    metric, rl_total_loss, int_reward, norm_int_reward = extra_info\n",
    "    return {\n",
    "        \"train_state\": runner_state[0],\n",
    "        \"metrics\": metric,\n",
    "        \"rl_total_loss\": rl_total_loss[0],\n",
    "        \"rl_value_loss\": rl_total_loss[1][0],\n",
    "        \"rl_actor_loss\": rl_total_loss[1][1],\n",
    "        \"rl_entrophy_loss\": rl_total_loss[1][2],\n",
    "        \"pred_loss\": rl_total_loss[2],\n",
    "        \"int_reward\": int_reward,\n",
    "        \"norm_int_reward\": norm_int_reward,\n",
    "        \"rng\": runner_state[-1]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def es_step(\n",
    "    train_fn,rng, train_state, pred_state, target_state, init_bt, close_init_hstate, open_init_hstate, init_action\n",
    "):\n",
    "    group=\"reward_combiners\"\n",
    "    tags=[\"meta-learner\", config[\"ENV_NAME\"]]\n",
    "    name=f'{config[\"RUN_NAME\"]}_{config[\"ENV_NAME\"]}'\n",
    "    # fit_log=wandb.init(\n",
    "    #             project=\"MetaLearnCuriosity\",\n",
    "    #             config=config,\n",
    "    #             group=group,\n",
    "    #             tags=tags,\n",
    "    #             name=f\"{name}_fitness\",\n",
    "    #         )\n",
    "    reward_combiner_network = TemporalRewardCombiner()\n",
    "    rc_params_pholder = reward_combiner_network.init(\n",
    "        jax.random.PRNGKey(config[\"RC_SEED\"]), jnp.zeros((1, 3))\n",
    "    )\n",
    "    es_rng = jax.random.PRNGKey(config[\"ES_SEED\"])\n",
    "    strategy = OpenES(\n",
    "        popsize=config[\"POP_SIZE\"],\n",
    "        pholder_params=rc_params_pholder,\n",
    "        opt_name=\"adam\",\n",
    "        lrate_init=2e-4,\n",
    "    )\n",
    "\n",
    "    es_rng, es_rng_init = jax.random.split(es_rng)\n",
    "    es_params = strategy.default_params\n",
    "    es_state = strategy.initialize(es_rng_init, es_params)\n",
    "\n",
    "    for _ in tqdm(range(config[\"NUM_GENERATIONS\"]), desc=\"Processing Generations\"):\n",
    "        t = time.time()\n",
    "        es_rng, es_rng_ask = jax.random.split(es_rng)\n",
    "        x, es_state = strategy.ask(es_rng_ask, es_state, es_params)\n",
    "        output = jax.block_until_ready(\n",
    "            train_fn(\n",
    "                rng,\n",
    "                x,\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                init_bt,\n",
    "                close_init_hstate,\n",
    "                open_init_hstate,\n",
    "                init_action,\n",
    "            )\n",
    "        )\n",
    "        rewards = output[\"metrics\"][\"sum_of_rewards\"]\n",
    "        rng_ = unreplicate(output[\"rng\"])\n",
    "        # (4, 2, 8, 1220, 128, 16)\n",
    "        fitness = rewards.mean(-1).mean(2).reshape(rewards.shape[0], rewards.shape[1], -1).sum(-1)\n",
    "        es_state = strategy.tell(x, fitness, es_state, es_params)\n",
    "        elapsed_time = time.time() - t\n",
    "        print(f\"Done in {elapsed_time / 60:.2f}min\")\n",
    "    #     fit_log.log({f\"{name}_mean_fitness\":fitness.mean(),\n",
    "    #                  f\"{name}_best_fitness\":jnp.max(fitness)})\n",
    "\n",
    "    # fit_log.finish()\n",
    "    # logger = WBLogger(\n",
    "    #     config=config,\n",
    "    #     group=group,\n",
    "    #     tags=tags,\n",
    "    #     name=name,\n",
    "    # )\n",
    "    # # Get the absolute path of the directory\n",
    "    # checkpoint_directory = f'MLC_logs/flax_ckpt/{config[\"ENV_NAME\"]}/{config[\"RUN_NAME\"]}'\n",
    "    params=strategy.param_reshaper.reshape_single(es_state.mean[0])\n",
    "    print(params)\n",
    "    # path = os.path.abspath(checkpoint_directory)\n",
    "    # Save(path, params)\n",
    "    # logger.save_artifact(path)\n",
    "    # shutil.rmtree(path)\n",
    "\n",
    "    return es_rng, es_state, fitness,output[\"rng\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in Asterix-MinAtar\n",
      "ParameterReshaper: 4 devices detected. Please make sure that the ES population size divides evenly across the number of devices to pmap/parallelize over.\n",
      "ParameterReshaper: 4481 parameters detected for optimization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Generations:   0%|          | 0/2 [00:00<?, ?it/s]/home/batsy/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "Processing Generations:  50%|█████     | 1/2 [12:36<12:36, 756.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 12.62min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Generations: 100%|██████████| 2/2 [24:37<00:00, 738.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done in 12.01min\n",
      "{'params': {'Dense_0': {'bias': Array([-1.6313215 ,  1.9180655 , -1.2958858 ,  1.5257062 , -1.6895043 ,\n",
      "       -0.3413339 , -1.9160405 ,  0.20504959,  0.30581132, -0.7549223 ,\n",
      "       -1.9819707 ,  1.9887234 , -1.920914  ,  1.3097045 ,  0.52386355,\n",
      "       -0.02650033,  0.83873504,  1.1383698 , -1.3117822 ,  0.6856074 ,\n",
      "       -0.9332975 ,  0.19743367,  1.4761066 , -1.7068906 ,  1.3362262 ,\n",
      "       -0.69882005,  0.18876372, -0.6923141 ,  0.6432712 ,  1.2847308 ,\n",
      "       -0.32504177,  0.01683885, -1.6880344 ,  1.7459917 ,  1.0336772 ,\n",
      "       -1.7787343 , -1.5043781 , -0.3447996 ,  1.7889901 ,  0.09623729,\n",
      "       -1.4789462 ,  0.38523203,  1.3221478 , -0.24969758,  0.7906623 ,\n",
      "        0.8117293 ,  0.50637513,  1.4963506 , -0.22705078,  0.9964611 ,\n",
      "        0.30855957, -1.3571128 ,  1.2790744 , -1.9557002 ,  0.7683513 ,\n",
      "       -0.67727995,  1.4581444 , -1.4872427 ,  1.9181707 , -1.683825  ,\n",
      "       -0.9087271 , -1.2258589 , -0.31389362,  1.736255  ], dtype=float32), 'kernel': Array([[ 1.65330970e+00,  3.66393954e-01, -1.23084784e+00,\n",
      "         1.85743451e+00,  5.28756261e-01, -3.50258619e-01,\n",
      "        -9.47308183e-01, -1.64054960e-01,  3.74041885e-01,\n",
      "        -7.60311186e-02,  1.56076443e+00, -3.09249312e-02,\n",
      "        -3.04570198e-01, -6.02792144e-01, -1.28990006e+00,\n",
      "        -1.23880231e+00, -1.22134292e+00,  2.00070691e+00,\n",
      "         4.84188721e-02, -2.76781861e-02, -1.44773185e+00,\n",
      "        -3.05647314e-01, -1.45725429e+00,  2.27674678e-01,\n",
      "        -1.52811038e+00,  6.51878595e-01,  7.45854139e-01,\n",
      "         5.04355282e-02,  1.36210394e+00, -9.36253190e-01,\n",
      "        -2.22136229e-01,  1.47114253e+00,  7.10617244e-01,\n",
      "         1.78947568e+00, -8.04061949e-01, -1.52381372e+00,\n",
      "         1.11255562e+00, -5.88994801e-01, -6.84197724e-01,\n",
      "         1.43767166e+00, -1.15889359e+00, -4.27045584e-01,\n",
      "        -7.47168183e-01, -1.78134954e+00,  3.05612665e-02,\n",
      "        -9.40849543e-01,  7.75675416e-01,  1.43802017e-01,\n",
      "        -7.09656999e-02, -1.30787504e+00, -5.25427580e-01,\n",
      "        -1.88052487e-02,  2.37269104e-01,  9.69998777e-01,\n",
      "        -3.21929187e-01, -1.46982193e+00,  1.68371117e+00,\n",
      "        -1.94587898e+00, -2.71829844e-01,  1.96852291e+00,\n",
      "        -1.90282941e+00,  7.71238953e-02, -3.56408000e-01,\n",
      "         1.17954266e+00],\n",
      "       [-1.30875278e+00,  3.60942692e-01,  1.88548779e+00,\n",
      "         4.50550109e-01, -1.10127091e+00, -6.50521144e-02,\n",
      "         1.98905778e+00,  1.15702403e+00,  1.70615271e-01,\n",
      "        -1.61520898e+00,  1.76522300e-01, -9.94995475e-01,\n",
      "         1.81466758e+00, -9.42610443e-01,  1.05622792e+00,\n",
      "        -1.62817764e+00,  1.06566918e+00,  1.24279499e+00,\n",
      "        -9.07085717e-01,  6.89599752e-01,  4.36898023e-01,\n",
      "         1.91861856e+00,  5.95447183e-01, -4.77552898e-02,\n",
      "        -1.42780578e+00, -1.75961661e+00,  1.02767205e+00,\n",
      "        -1.55517328e+00, -1.44013369e+00,  1.69510651e+00,\n",
      "        -1.14621925e+00,  1.04799652e+00, -1.83002353e+00,\n",
      "         1.85155046e+00, -7.64840245e-01, -5.77029765e-01,\n",
      "         5.90918481e-01, -1.22044337e+00,  1.42094839e+00,\n",
      "         1.91837728e-01,  1.88173628e+00,  6.62784517e-01,\n",
      "        -9.95762289e-01,  1.23569381e+00, -1.79263663e+00,\n",
      "        -1.11877441e+00, -8.13855469e-01,  9.87117767e-01,\n",
      "         1.27142012e-01,  1.39164221e+00,  1.75425279e+00,\n",
      "         5.97201526e-01,  9.93669033e-01,  1.88663077e+00,\n",
      "        -6.95160925e-01, -9.77033317e-01,  5.17369568e-01,\n",
      "        -7.42413282e-01, -1.58961987e+00, -4.84504491e-01,\n",
      "        -1.18814921e+00, -5.74211061e-01, -2.79123008e-01,\n",
      "         8.80273342e-01],\n",
      "       [-8.07231545e-01,  7.39433885e-01,  1.47280133e+00,\n",
      "        -5.87379396e-01, -1.23751044e+00,  1.20351100e+00,\n",
      "        -1.21778095e+00, -7.11305320e-01,  5.30883670e-01,\n",
      "        -9.90451217e-01, -1.56617427e+00,  3.58698040e-01,\n",
      "        -4.43111211e-01,  9.80420411e-01, -1.82705581e+00,\n",
      "         1.49901152e+00, -4.27295417e-01, -7.91522413e-02,\n",
      "         7.01910734e-01, -9.95344937e-01,  9.83411312e-01,\n",
      "        -5.82141459e-01, -1.87643313e+00, -1.25065601e+00,\n",
      "        -1.30601621e+00,  1.36021301e-01,  5.45735180e-01,\n",
      "         1.68355510e-01,  1.10061479e+00,  9.44483697e-01,\n",
      "         1.63429177e+00, -1.04739857e+00,  1.25440389e-01,\n",
      "         1.15921065e-01,  1.24998999e+00, -1.35774088e+00,\n",
      "        -1.42601824e+00,  1.85977757e+00,  1.36555612e+00,\n",
      "         1.51949728e+00,  8.49749804e-01,  4.89818186e-01,\n",
      "        -5.13906419e-01,  9.08988044e-02,  1.78343500e-03,\n",
      "        -1.70248795e+00,  1.41184366e+00,  3.82361323e-01,\n",
      "        -9.64714587e-01,  1.22956669e+00,  1.94026971e+00,\n",
      "        -1.72480011e+00, -1.72139871e+00,  9.43081081e-01,\n",
      "        -9.49747562e-01,  9.25257862e-01, -6.89909577e-01,\n",
      "         9.44362342e-01,  1.87420738e+00, -5.80860436e-01,\n",
      "         1.07384193e+00, -1.85635701e-01, -1.16931312e-01,\n",
      "        -1.01798630e+00]], dtype=float32)}, 'Dense_1': {'bias': Array([ 9.8139471e-01, -1.6415135e+00,  1.9089934e+00,  1.8745341e+00,\n",
      "        1.7290330e+00,  7.2179019e-01, -7.6767826e-01,  1.7305400e+00,\n",
      "       -1.0303209e+00, -8.6714482e-01, -1.6775645e+00, -1.3496220e+00,\n",
      "        4.4916535e-04, -6.8540937e-01,  1.6998788e+00, -1.7581608e+00,\n",
      "       -5.7768536e-01, -1.0779549e+00, -1.2142819e+00,  1.7794186e+00,\n",
      "        1.8484880e+00,  1.5761507e+00, -1.4120879e+00, -5.6700510e-01,\n",
      "       -2.4723044e-01,  4.1216549e-01, -1.6635330e-01, -1.5281185e+00,\n",
      "       -1.5709267e+00,  1.3416488e+00, -6.2972724e-02, -2.3926291e-01,\n",
      "       -1.7201644e+00, -2.4704094e-01, -1.2391098e+00,  1.6105664e+00,\n",
      "       -7.1485865e-01,  1.8493631e+00, -1.1763210e+00, -2.4801688e-01,\n",
      "        1.4486905e+00,  3.5831943e-01,  1.1825441e+00,  1.6030382e+00,\n",
      "       -1.1447583e+00, -3.0854547e-01, -1.3584569e+00, -4.8529565e-01,\n",
      "       -5.7298556e-02, -1.4171294e+00, -5.8595413e-01,  1.4256424e+00,\n",
      "       -3.5844150e-01, -1.3850694e+00,  6.1338162e-01, -1.1539624e+00,\n",
      "        6.9450909e-01, -9.6587890e-01,  1.4180626e+00, -1.5587847e+00,\n",
      "        1.6714061e+00, -8.6344165e-01, -9.9951655e-01,  1.7977288e+00],      dtype=float32), 'kernel': Array([[-1.9717408 ,  0.6590586 , -0.26810122, ..., -1.4147063 ,\n",
      "        -0.4747292 , -1.5826254 ],\n",
      "       [ 1.8369278 , -0.9805767 ,  0.21330808, ..., -1.0280776 ,\n",
      "         0.68783003, -0.97735167],\n",
      "       [ 1.3788071 , -1.7742755 , -1.5065106 , ...,  1.6022985 ,\n",
      "         0.10877647, -1.4254209 ],\n",
      "       ...,\n",
      "       [-1.7405876 , -0.3885108 , -0.48541087, ..., -1.1642405 ,\n",
      "        -1.3360844 ,  1.4383672 ],\n",
      "       [-0.3835034 ,  0.6855451 , -0.0921205 , ..., -0.2587381 ,\n",
      "        -1.4134183 , -0.9138485 ],\n",
      "       [ 1.0594481 ,  1.9970402 ,  0.10377302, ...,  1.727778  ,\n",
      "         1.6011581 ,  1.6829066 ]], dtype=float32)}, 'Dense_2': {'bias': Array([-0.7400407], dtype=float32), 'kernel': Array([[-0.87464577],\n",
      "       [ 1.4136412 ],\n",
      "       [-0.3861418 ],\n",
      "       [-0.8703887 ],\n",
      "       [-1.9270871 ],\n",
      "       [ 0.14715658],\n",
      "       [-1.6284778 ],\n",
      "       [ 1.9900329 ],\n",
      "       [-1.5273713 ],\n",
      "       [-1.275832  ],\n",
      "       [ 0.40622002],\n",
      "       [ 1.9605814 ],\n",
      "       [-1.911051  ],\n",
      "       [ 1.501584  ],\n",
      "       [-1.2233424 ],\n",
      "       [ 1.1564898 ],\n",
      "       [ 0.63226455],\n",
      "       [ 1.325565  ],\n",
      "       [-1.2632499 ],\n",
      "       [-0.51467735],\n",
      "       [-0.00747646],\n",
      "       [ 0.15004969],\n",
      "       [-0.29105753],\n",
      "       [-0.1347603 ],\n",
      "       [ 1.0060104 ],\n",
      "       [-0.0022856 ],\n",
      "       [-0.21982259],\n",
      "       [ 1.0196072 ],\n",
      "       [-0.4876116 ],\n",
      "       [ 1.1536742 ],\n",
      "       [-1.9052166 ],\n",
      "       [ 1.9717605 ],\n",
      "       [ 1.4570532 ],\n",
      "       [-0.29534358],\n",
      "       [ 0.26746207],\n",
      "       [-1.1548704 ],\n",
      "       [-1.1702371 ],\n",
      "       [-0.52976966],\n",
      "       [ 1.880867  ],\n",
      "       [ 0.19620344],\n",
      "       [-0.21183725],\n",
      "       [ 1.9857041 ],\n",
      "       [-0.92846483],\n",
      "       [-1.8764044 ],\n",
      "       [-1.1564833 ],\n",
      "       [-1.6561222 ],\n",
      "       [-1.26622   ],\n",
      "       [-0.3692939 ],\n",
      "       [-0.7976272 ],\n",
      "       [-0.25818422],\n",
      "       [-0.05044427],\n",
      "       [-1.6279281 ],\n",
      "       [-0.3762761 ],\n",
      "       [-0.4558089 ],\n",
      "       [-1.400559  ],\n",
      "       [ 1.9056212 ],\n",
      "       [ 0.44493645],\n",
      "       [-0.18455434],\n",
      "       [ 0.01985373],\n",
      "       [-1.6380991 ],\n",
      "       [-0.75955623],\n",
      "       [-0.03001663],\n",
      "       [-1.1333481 ],\n",
      "       [-0.33980304]], dtype=float32)}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for env_name in environments:\n",
    "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "    config, env, env_params = make_config_env(config, env_name)\n",
    "    print(f\"Training in {config['ENV_NAME']}\")\n",
    "\n",
    "    rng = jax.random.split(rng, config[\"NUM_SEEDS\"])\n",
    "    (\n",
    "        rng,\n",
    "        train_state,\n",
    "        pred_state,\n",
    "        target_state,\n",
    "        init_bt,\n",
    "        close_init_hstate,\n",
    "        open_init_hstate,\n",
    "        init_action,\n",
    "    ) = jax.jit(jax.vmap(ppo_make_train, out_axes=(1, 0, 0, 0, 0, 0, 0, 0)))(rng)\n",
    "    open_init_hstate = replicate(open_init_hstate, jax.local_devices())\n",
    "    close_init_hstate = replicate(close_init_hstate, jax.local_devices())\n",
    "    train_state = replicate(train_state, jax.local_devices())\n",
    "    pred_state = replicate(pred_state, jax.local_devices())\n",
    "    target_state = replicate(target_state, jax.local_devices())\n",
    "    init_bt = replicate(init_bt, jax.local_devices())\n",
    "    init_action = replicate(init_action, jax.local_devices())\n",
    "    train_fn = jax.vmap(train, in_axes=(0, None,0,0,0,0,0,0,0))\n",
    "    train_fn = jax.vmap(train_fn, in_axes=(None, 0,None,None,None,None,None,None,None))\n",
    "    train_fn = jax.pmap(train_fn, axis_name=\"devices\")\n",
    "    # Set up fitness function\n",
    "\n",
    "f,j,k,m=es_step(\n",
    "        train_fn, rng, train_state, pred_state, target_state, init_bt, close_init_hstate, open_init_hstate, init_action\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 8, 2)\n"
     ]
    }
   ],
   "source": [
    "print(rng.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpu_curiosax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
