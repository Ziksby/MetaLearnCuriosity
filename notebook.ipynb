{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from typing import Any, NamedTuple, Sequence\n",
    "\n",
    "import distrax\n",
    "import flax.linen as nn\n",
    "import gymnax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from MetaLearnCuriosity.agents.nn import PredictorNetwork, TargetNetwork\n",
    "from MetaLearnCuriosity.checkpoints import Save\n",
    "from MetaLearnCuriosity.logger import WBLogger\n",
    "from MetaLearnCuriosity.utils import (\n",
    "    ObsNormParams,\n",
    "    RNDNormIntReturnParams,\n",
    "    RNDTransition,\n",
    "    make_obs_gymnax_discrete,\n",
    "    process_output_general,\n",
    "    rnd_normalise_int_rewards,\n",
    "    update_obs_norm_params,\n",
    ")\n",
    "from MetaLearnCuriosity.wrappers import FlattenObservationWrapper, LogWrapper, VecEnv\n",
    "\n",
    "\n",
    "class PPOActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
    "            actor_mean\n",
    "        )\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        critic = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(critic)\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "# control problems\n",
    "# config = {\n",
    "#     \"RUN_NAME\": \"dis_ppo\",\n",
    "#     \"SEED\": 42,\n",
    "#     \"NUM_SEEDS\": 30,\n",
    "#     \"LR\": 2.5e-4,\n",
    "#     \"NUM_ENVS\": 4,\n",
    "#     \"NUM_STEPS\": 128,\n",
    "#     \"TOTAL_TIMESTEPS\": 5e5,\n",
    "#     \"UPDATE_EPOCHS\": 4,\n",
    "#     \"NUM_MINIBATCHES\": 4,\n",
    "#     \"GAMMA\": 0.99,\n",
    "#     \"GAE_LAMBDA\": 0.95,\n",
    "#     \"CLIP_EPS\": 0.2,\n",
    "#     \"ENT_COEF\": 0.01,\n",
    "#     \"VF_COEF\": 0.5,\n",
    "#     \"MAX_GRAD_NORM\": 0.5,\n",
    "#     \"ACTIVATION\": \"tanh\",\n",
    "#     \"ENV_NAME\": \"Empty-misc\",\n",
    "#     \"ANNEAL_LR\": False,\n",
    "#     \"DEBUG\": False,\n",
    "# }\n",
    "\n",
    "# min atar\n",
    "\n",
    "config = {\n",
    "    \"RUN_NAME\": \"minatar_rnd\",\n",
    "    \"SEED\": 42,\n",
    "    \"NUM_SEEDS\": 10,\n",
    "    \"LR\": 5e-3,\n",
    "    \"PRED_LR\": 1e-3,\n",
    "    \"NUM_ENVS\": 64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"INT_GAMMA\": 0.99,\n",
    "    # \"INT_LAMBDA\": 0.1,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"DEBUG\": False,\n",
    "}\n",
    "\n",
    "environments = [\n",
    "    \"Asterix-MinAtar\",\n",
    "    # \"Breakout-MinAtar\",\n",
    "    # \"Freeway-MinAtar\",\n",
    "    # \"SpaceInvaders-MinAtar\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_config_env(config, env_name):\n",
    "    config[\"ENV_NAME\"] = env_name\n",
    "    num_devices = jax.local_device_count()\n",
    "    assert config[\"NUM_ENVS\"] % num_devices == 0\n",
    "    config[\"NUM_ENVS_PER_DEVICE\"] = config[\"NUM_ENVS\"] // num_devices\n",
    "    config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] = config[\"TOTAL_TIMESTEPS\"] // num_devices\n",
    "    # config[\"EVAL_EPISODES_PER_DEVICE\"] = config[\"EVAL_EPISODES\"] // num_devices\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS_PER_DEVICE\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    config[\"TRAINING_HORIZON\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] // config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "    )\n",
    "\n",
    "    assert config[\"NUM_ENVS_PER_DEVICE\"] >= 4\n",
    "    config[\"UPDATE_PROPORTION\"] = 4 / config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "    env = VecEnv(env)\n",
    "\n",
    "    return config, env, env_params\n",
    "\n",
    "\n",
    "def ppo_make_train(rng):\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def pred_linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"PRED_LR\"] * frac\n",
    "\n",
    "    # INIT NETWORKS\n",
    "    network = PPOActorCritic(env.action_space(env_params).n, activation=config[\"ACTIVATION\"])\n",
    "    target = TargetNetwork(64)\n",
    "    predictor = PredictorNetwork(64)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    rng, _pred_rng = jax.random.split(rng)\n",
    "    rng, _tar_rng = jax.random.split(rng)\n",
    "    rng, _init_obs_rng = jax.random.split(rng)\n",
    "\n",
    "    init_x = jnp.zeros(env.observation_space(env_params).shape)\n",
    "    network_params = network.init(_rng, init_x)\n",
    "    target_params = target.init(_tar_rng, init_x)\n",
    "    pred_params = predictor.init(_pred_rng, init_x)\n",
    "\n",
    "    if config[\"ANNEAL_LR\"]:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "        )\n",
    "    else:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(config[\"LR\"], eps=1e-5),\n",
    "        )\n",
    "    train_state = TrainState.create(\n",
    "        apply_fn=network.apply,\n",
    "        params=network_params,\n",
    "        tx=tx,\n",
    "    )\n",
    "\n",
    "    pred_tx = optax.chain(\n",
    "        optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "        optax.adam(config[\"PRED_LR\"], eps=1e-5),\n",
    "    )\n",
    "\n",
    "    predictor_state = TrainState.create(apply_fn=predictor.apply, params=pred_params, tx=pred_tx)\n",
    "\n",
    "    rng = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "    return rng, train_state, predictor_state, target_params, _init_obs_rng\n",
    "\n",
    "\n",
    "def train(rng, train_state, pred_state, target_params, init_obs_rng):\n",
    "\n",
    "    # INIT OBS NORM PARAMS:\n",
    "    random_rollout = make_obs_gymnax_discrete(\n",
    "        config[\"NUM_ENVS_PER_DEVICE\"], env, env_params, config[\"NUM_STEPS\"]\n",
    "    )\n",
    "\n",
    "    # Obs will be in shape: num_steps, num_envs, obs.shape\n",
    "    init_obs = random_rollout(init_obs_rng)\n",
    "    init_obs = init_obs.reshape(\n",
    "        -1, init_obs.shape[-1]\n",
    "    )  # reshape it to num_envs*num_steps, obs.shape\n",
    "\n",
    "    init_mean_obs = jnp.zeros(init_obs.shape[-1])\n",
    "    init_var_obs = jnp.ones(init_obs.shape[-1])\n",
    "    init_obs_count = 1e-4\n",
    "\n",
    "    init_obs_norm_params = ObsNormParams(init_obs_count, init_mean_obs, init_var_obs)\n",
    "    rnd_int_return_norm_params = RNDNormIntReturnParams(\n",
    "        1e-4, 0.0, 1.0, jnp.zeros((config[\"NUM_STEPS\"],))\n",
    "    )\n",
    "\n",
    "    obs_norm_params = update_obs_norm_params(init_obs_norm_params, init_obs)\n",
    "    target = TargetNetwork(64)\n",
    "\n",
    "    # INIT ENV\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "    obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "    # TRAIN LOOP\n",
    "    def _update_step(runner_state, unused):\n",
    "        # COLLECT TRAJECTORIES\n",
    "        def _env_step(runner_state, unused):\n",
    "            (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_params,\n",
    "                env_state,\n",
    "                last_obs,\n",
    "                rnd_int_return_norm_params,\n",
    "                obs_norm_params,\n",
    "                rng,\n",
    "            ) = runner_state\n",
    "\n",
    "            # SELECT ACTION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            pi, value = train_state.apply_fn(train_state.params, last_obs)\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # STEP ENV\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            rng_step = jax.random.split(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "            obsv, env_state, reward, done, info = env.step(rng_step, env_state, action, env_params)\n",
    "\n",
    "            # NORM THE OBS\n",
    "            rnd_obs = ((obsv - obs_norm_params.mean) / jnp.sqrt(obs_norm_params.var)).clip(-5, 5)\n",
    "\n",
    "            # INT REWARD\n",
    "            tar_feat = target.apply(target_params, rnd_obs)\n",
    "            pred_feat = pred_state.apply_fn(pred_state.params, rnd_obs)\n",
    "            int_reward = jnp.square(jnp.linalg.norm((pred_feat - tar_feat), axis=1)) / 2\n",
    "\n",
    "            transition = RNDTransition(\n",
    "                done, action, value, reward, int_reward, log_prob, last_obs, info\n",
    "            )\n",
    "            runner_state = (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_params,\n",
    "                env_state,\n",
    "                obsv,\n",
    "                rnd_int_return_norm_params,\n",
    "                obs_norm_params,\n",
    "                rng,\n",
    "            )\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state, traj_batch = jax.lax.scan(_env_step, runner_state, None, config[\"NUM_STEPS\"])\n",
    "\n",
    "        # CALCULATE ADVANTAGE AND NORMALISE INT REWARDS\n",
    "        (\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            target_params,\n",
    "            env_state,\n",
    "            last_obs,\n",
    "            rnd_int_return_norm_params,\n",
    "            obs_norm_params,\n",
    "            rng,\n",
    "        ) = runner_state\n",
    "        _, last_val = train_state.apply_fn(train_state.params, last_obs)\n",
    "\n",
    "        def _calculate_gae(traj_batch, last_val, rnd_int_return_norm_params):\n",
    "\n",
    "            norm_int_reward, rnd_int_return_norm_params = rnd_normalise_int_rewards(\n",
    "                traj_batch, rnd_int_return_norm_params, config[\"INT_GAMMA\"]\n",
    "            )\n",
    "\n",
    "            norm_traj_batch = RNDTransition(\n",
    "                traj_batch.done,\n",
    "                traj_batch.action,\n",
    "                traj_batch.value,\n",
    "                traj_batch.reward,\n",
    "                norm_int_reward,\n",
    "                traj_batch.log_prob,\n",
    "                traj_batch.obs,\n",
    "                traj_batch.info,\n",
    "            )\n",
    "\n",
    "            def _get_advantages(gae_and_next_value, transition):\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward, int_reward = (\n",
    "                    transition.done,\n",
    "                    transition.value,\n",
    "                    transition.reward,\n",
    "                    transition.int_reward,\n",
    "                )\n",
    "                delta = (\n",
    "                    (reward + (config[\"INT_LAMBDA\"] * int_reward))\n",
    "                    + config[\"GAMMA\"] * next_value * (1 - done)\n",
    "                    - value\n",
    "                )\n",
    "                gae = delta + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                norm_traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return (\n",
    "                advantages,\n",
    "                advantages + traj_batch.value,\n",
    "                norm_int_reward,\n",
    "                rnd_int_return_norm_params,\n",
    "            )\n",
    "\n",
    "        advantages, targets, norm_int_rewards, rnd_int_return_norm_params = _calculate_gae(\n",
    "            traj_batch, last_val, rnd_int_return_norm_params\n",
    "        )\n",
    "\n",
    "        # UPDATE NETWORK\n",
    "        def _update_epoch(update_state, unused):\n",
    "\n",
    "            (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                traj_batch,\n",
    "                advantages,\n",
    "                targets,\n",
    "                obs_norm_params,\n",
    "                rng,\n",
    "            ) = update_state\n",
    "            rng, _mask_rng = jax.random.split(rng)\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "\n",
    "            def _update_minbatch(network_states, batch_info):\n",
    "                traj_batch, advantages, targets, rnd_obs = batch_info\n",
    "                train_state, pred_state = network_states\n",
    "\n",
    "                def _rnd_loss(pred_params, rnd_obs):\n",
    "                    tar_feat = target.apply(target_params, rnd_obs)\n",
    "                    pred_feat = pred_state.apply_fn(pred_params, rnd_obs)\n",
    "                    loss = jnp.square(jnp.linalg.norm((pred_feat - tar_feat), axis=1)) / 2\n",
    "\n",
    "                    mask = jax.random.uniform(_mask_rng, (loss.shape[0],))\n",
    "                    mask = (mask < config[\"UPDATE_PROPORTION\"]).astype(jnp.float32)\n",
    "                    loss = loss * mask\n",
    "                    return loss.sum() / jnp.max(jnp.array([mask.sum(), 1]))\n",
    "\n",
    "                def _loss_fn(params, traj_batch, gae, targets):\n",
    "                    # RERUN NETWORK\n",
    "                    pi, value = train_state.apply_fn(params, traj_batch.obs)\n",
    "                    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                    # CALCULATE VALUE LOSS\n",
    "                    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "                        -config[\"CLIP_EPS\"], config[\"CLIP_EPS\"]\n",
    "                    )\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "                    # CALCULATE ACTOR LOSS\n",
    "                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = (\n",
    "                        jnp.clip(\n",
    "                            ratio,\n",
    "                            1.0 - config[\"CLIP_EPS\"],\n",
    "                            1.0 + config[\"CLIP_EPS\"],\n",
    "                        )\n",
    "                        * gae\n",
    "                    )\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                    loss_actor = loss_actor.mean()\n",
    "                    entropy = pi.entropy().mean()\n",
    "\n",
    "                    total_loss = (\n",
    "                        loss_actor + config[\"VF_COEF\"] * value_loss - config[\"ENT_COEF\"] * entropy\n",
    "                    )\n",
    "                    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                (loss, (vloss, aloss, entropy)), grads = jax.value_and_grad(_loss_fn, has_aux=True)(\n",
    "                    train_state.params, traj_batch, advantages, targets\n",
    "                )\n",
    "\n",
    "                rnd_loss, rnd_grads = jax.value_and_grad(_rnd_loss)(pred_state.params, rnd_obs)\n",
    "                (loss, vloss, aloss, entropy, rnd_loss, grads, rnd_grads) = jax.lax.pmean(\n",
    "                    (loss, vloss, aloss, entropy, rnd_loss, grads, rnd_grads), axis_name=\"devices\"\n",
    "                )\n",
    "                train_state = train_state.apply_gradients(grads=grads)\n",
    "                pred_state = pred_state.apply_gradients(grads=rnd_grads)\n",
    "                return (train_state, pred_state), (loss, (vloss, aloss, entropy, rnd_loss))\n",
    "\n",
    "            # UPDATE OBS NORM PARAMETERS\n",
    "            obs_norm_params = update_obs_norm_params(\n",
    "                obs_norm_params, traj_batch.obs.reshape(-1, init_obs.shape[-1])\n",
    "            )\n",
    "            # GET RND OBS\n",
    "            rnd_obs = (\n",
    "                (traj_batch.obs - obs_norm_params.mean) / jnp.sqrt(obs_norm_params.var)\n",
    "            ).clip(-5, 5)\n",
    "\n",
    "            batch_size = config[\"MINIBATCH_SIZE\"] * config[\"NUM_MINIBATCHES\"]\n",
    "            assert (\n",
    "                batch_size == (config[\"NUM_STEPS\"]) * config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "            ), \"batch size must be equal to number of steps * number of envs\"\n",
    "            permutation = jax.random.permutation(_rng, batch_size)\n",
    "            batch = (traj_batch, advantages, targets, rnd_obs)\n",
    "            batch = jax.tree_util.tree_map(lambda x: x.reshape((batch_size,) + x.shape[2:]), batch)\n",
    "            shuffled_batch = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=0), batch\n",
    "            )\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.reshape(x, [config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[1:])),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "            (train_state, pred_state), total_loss = jax.lax.scan(\n",
    "                _update_minbatch, (train_state, pred_state), minibatches\n",
    "            )\n",
    "            update_state = (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                traj_batch,\n",
    "                advantages,\n",
    "                targets,\n",
    "                obs_norm_params,\n",
    "                rng,\n",
    "            )\n",
    "            return update_state, total_loss\n",
    "\n",
    "        update_state = (\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            traj_batch,\n",
    "            advantages,\n",
    "            targets,\n",
    "            obs_norm_params,\n",
    "            rng,\n",
    "        )\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "        )\n",
    "        train_state = update_state[0]\n",
    "        pred_state = update_state[1]\n",
    "        obs_norm_params = update_state[-2]\n",
    "        metric = traj_batch.info\n",
    "        rng = update_state[-1]\n",
    "        if config.get(\"DEBUG\"):\n",
    "\n",
    "            def callback(info):\n",
    "                return_values = info[\"returned_episode_returns\"][info[\"returned_episode\"]]\n",
    "                timesteps = (\n",
    "                    info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "                )\n",
    "                for t in range(len(timesteps)):\n",
    "                    print(f\"global step={timesteps[t]}, episodic return={return_values[t]}\")\n",
    "\n",
    "            jax.debug.callback(callback, metric)\n",
    "\n",
    "        runner_state = (\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            target_params,\n",
    "            env_state,\n",
    "            last_obs,\n",
    "            rnd_int_return_norm_params,\n",
    "            obs_norm_params,\n",
    "            rng,\n",
    "        )\n",
    "        return runner_state, (metric, traj_batch.int_reward, norm_int_rewards, loss_info)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (\n",
    "        train_state,\n",
    "        pred_state,\n",
    "        target_params,\n",
    "        env_state,\n",
    "        obsv,\n",
    "        rnd_int_return_norm_params,\n",
    "        obs_norm_params,\n",
    "        _rng,\n",
    "    )\n",
    "    runner_state, extra_info = jax.lax.scan(_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
    "    metric, int_rewards, norm_int_rewards, rl_total_loss = extra_info\n",
    "    return {\n",
    "        \"train_state\": runner_state[:3],\n",
    "        \"metrics\": metric,\n",
    "        \"int_reward\": int_rewards,\n",
    "        \"norm_int_reward\": norm_int_rewards,\n",
    "        \"rl_total_loss\": rl_total_loss[0],\n",
    "        \"rl_value_loss\": rl_total_loss[1][0],\n",
    "        \"rl_actor_loss\": rl_total_loss[1][1],\n",
    "        \"rl_entrophy_loss\": rl_total_loss[1][2],\n",
    "        \"rnd_loss\": rl_total_loss[1][3],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_values=jnp.array([ 000.1,0.01,0.1, 0.003,0.005,0.02,0.03,0.05,0.2,0.5]).sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in Asterix-MinAtar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "245.80579018592834\n",
      "(156160,) (4, 10, 1220, 128, 16)\n",
      "Training in Asterix-MinAtar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.4280698299408\n",
      "(156160,) (4, 10, 1220, 128, 16)\n",
      "Training in Asterix-MinAtar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.7095377445221\n",
      "(156160,) (4, 10, 1220, 128, 16)\n",
      "Training in Asterix-MinAtar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247.05615711212158\n",
      "(156160,) (4, 10, 1220, 128, 16)\n",
      "Training in Asterix-MinAtar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.8234350681305\n",
      "(156160,) (4, 10, 1220, 128, 16)\n",
      "Training in Asterix-MinAtar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246.79781293869019\n",
      "(156160,) (4, 10, 1220, 128, 16)\n",
      "Training in Asterix-MinAtar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int32 to dtype=bool with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.\n",
      "  warnings.warn(\n",
      "/home/batsy/masters/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:68: UserWarning: Explicitly requested dtype <class 'jax.numpy.int64'> requested in astype is not available, and will be truncated to dtype int32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return lax_numpy.astype(arr, dtype, copy=copy, device=device)\n"
     ]
    }
   ],
   "source": [
    "y_values = {}\n",
    "for lambda_value in lambda_values:\n",
    "    y_values[float(lambda_value)] = {}  # Use float(lambda_value) to ensure dictionary keys are serializable\n",
    "    config[\"INT_LAMBDA\"]=lambda_value\n",
    "    for env_name in environments:\n",
    "        rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "        t = time.time()\n",
    "        config, env, env_params = make_config_env(config, env_name)\n",
    "        print(f\"Training in {config['ENV_NAME']}\")\n",
    "\n",
    "        if config[\"NUM_SEEDS\"] > 1:\n",
    "            rng = jax.random.split(rng, config[\"NUM_SEEDS\"])\n",
    "            rng, train_state, pred_state, target_params, init_rnd_obs = jax.vmap(ppo_make_train, out_axes=(1, 0, 0, 0, 0))(rng)\n",
    "            train_state = replicate(train_state, jax.local_devices())\n",
    "            pred_state = replicate(pred_state, jax.local_devices())\n",
    "            target_params = replicate(target_params, jax.local_devices())\n",
    "            init_rnd_obs = replicate(init_rnd_obs, jax.local_devices())\n",
    "            train_fn = jax.vmap(train, in_axes=(0, 0, 0, 0, 0))\n",
    "            train_fn = jax.pmap(train_fn, axis_name=\"devices\")\n",
    "            output = jax.block_until_ready(train_fn(rng, train_state, pred_state, target_params, init_rnd_obs))\n",
    "\n",
    "        else:\n",
    "            rng, train_state, pred_state, target_params, init_rnd_obs = ppo_make_train(rng)\n",
    "            train_state = replicate(train_state, jax.local_devices())\n",
    "            pred_state = replicate(pred_state, jax.local_devices())\n",
    "            target_params = replicate(target_params, jax.local_devices())\n",
    "            init_rnd_obs = replicate(init_rnd_obs, jax.local_devices())\n",
    "            train_fn = jax.pmap(train, axis_name=\"devices\")\n",
    "            output = jax.block_until_ready(train_fn(rng, train_state, pred_state, target_params, init_rnd_obs, lambda_replicated))\n",
    "\n",
    "        print(time.time() - t)\n",
    "        # Assuming `output` is your array\n",
    "        epi_ret = output[\"metrics\"][\"returned_episode_returns\"].mean(0).mean(0).mean(-1).reshape(-1)\n",
    "        print(epi_ret.shape, output[\"metrics\"][\"returned_episode_returns\"].shape)\n",
    "\n",
    "        # Use the last element of each row from 'epi_ret' as y-values\n",
    "        y_values[float(lambda_value)][env_name] = epi_ret\n",
    "\n",
    "# Print or process `y_values` as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot for each environment\n",
    "for env_name in environments:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for lambda_value, env_data in y_values.items():\n",
    "        plt.plot(env_data[env_name], label=f'Lambda = {lambda_value:.5f}')\n",
    "    \n",
    "    plt.title(f'Episode Return Over Time for {env_name}')\n",
    "    plt.xlabel('Update Step')\n",
    "    plt.ylabel('Episode Return')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot for each environment\n",
    "for env_name in environments:\n",
    "    lambda_values = []\n",
    "    final_returns = []\n",
    "\n",
    "    for lambda_value, env_data in y_values.items():\n",
    "        final_return = env_data[env_name][-1]  # Get the last episode return\n",
    "        lambda_values.append(lambda_value)\n",
    "        final_returns.append(final_return)\n",
    "\n",
    "    # Generate x values as numerical indices from 1 to len(lambda_values)\n",
    "    x_values = list(range(1, len(lambda_values) + 1))\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(x_values, final_returns, color='blue')\n",
    "    plt.title(f'Final Episode Return vs. Lambda for {env_name}')\n",
    "    plt.xlabel('Lambda Value')\n",
    "    plt.ylabel('Final Episode Return')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(x_values, [f'{lv:.5f}' for lv in sorted(lambda_values)])  # Set x-ticks to formatted lambda values\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpu_curiosax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
