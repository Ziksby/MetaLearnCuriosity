{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/batsy/MetaLearnCuriosity/tpu_curiosax/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training in Asterix-MinAtar\n",
      "ParameterReshaper: 8 devices detected. Please make sure that the ES population size divides evenly across the number of devices to pmap/parallelize over.\n",
      "ParameterReshaper: 4481 parameters detected for optimization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Generations:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Taken from:\n",
    "# https://github.com/corl-team/xland-minigrid/blob/main/training/train_single_task.py\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from typing import Sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "import distrax\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import gymnax\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.tree_util\n",
    "import numpy as np\n",
    "import optax\n",
    "import wandb\n",
    "from flax.jax_utils import replicate, unreplicate\n",
    "from flax.linen.initializers import constant, orthogonal\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "from MetaLearnCuriosity.agents.nn import (\n",
    "    AtariBYOLPredictor,\n",
    "    BYOLTarget,\n",
    "    CloseScannedRNN,\n",
    "    OpenScannedRNN,\n",
    "    TemporalRewardCombiner\n",
    ")\n",
    "from MetaLearnCuriosity.pmapped_open_es import OpenES\n",
    "from MetaLearnCuriosity.checkpoints import Save\n",
    "from MetaLearnCuriosity.logger import WBLogger\n",
    "from MetaLearnCuriosity.utils import BYOLRewardNorm\n",
    "from MetaLearnCuriosity.utils import RCBYOLTransition as Transition\n",
    "from MetaLearnCuriosity.utils import (\n",
    "    byol_normalize_prior_int_rewards,\n",
    "    process_output_general,\n",
    "    update_target_state_with_ema,\n",
    ")\n",
    "from MetaLearnCuriosity.wrappers import FlattenObservationWrapper, LogWrapper, VecEnv\n",
    "\n",
    "environments = [\n",
    "    \"Asterix-MinAtar\",\n",
    "    # \"Breakout-MinAtar\",\n",
    "    # \"Freeway-MinAtar\",\n",
    "    # \"SpaceInvaders-MinAtar\",\n",
    "]\n",
    "\n",
    "\n",
    "class PPOActorCritic(nn.Module):\n",
    "    action_dim: Sequence[int]\n",
    "    activation: str = \"tanh\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        if self.activation == \"relu\":\n",
    "            activation = nn.relu\n",
    "        else:\n",
    "            activation = nn.tanh\n",
    "        actor_mean = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
    "            actor_mean\n",
    "        )\n",
    "        actor_mean = activation(actor_mean)\n",
    "        actor_mean = nn.Dense(\n",
    "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
    "        )(actor_mean)\n",
    "        pi = distrax.Categorical(logits=actor_mean)\n",
    "\n",
    "        critic = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(critic)\n",
    "        critic = activation(critic)\n",
    "        critic = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(critic)\n",
    "\n",
    "        return pi, jnp.squeeze(critic, axis=-1)\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"RUN_NAME\": \"minatar_byol_ppo\",\n",
    "    \"SEED\": 42,\n",
    "    \"NUM_SEEDS\": 10,\n",
    "    \"LR\": 5e-3,\n",
    "    \"NUM_ENVS\": 64,\n",
    "    \"NUM_STEPS\": 128,\n",
    "    \"TOTAL_TIMESTEPS\": 1e7,\n",
    "    \"UPDATE_EPOCHS\": 4,\n",
    "    \"NUM_MINIBATCHES\": 8,\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"CLIP_EPS\": 0.2,\n",
    "    \"ENT_COEF\": 0.01,\n",
    "    \"VF_COEF\": 0.5,\n",
    "    \"MAX_GRAD_NORM\": 0.5,\n",
    "    \"ACTIVATION\": \"relu\",\n",
    "    \"ANNEAL_LR\": True,\n",
    "    \"ANNEAL_PRED_LR\": False,\n",
    "    \"DEBUG\": False,\n",
    "    \"PRED_LR\": 0.001,\n",
    "    \"REW_NORM_PARAMETER\": 0.99,\n",
    "    \"EMA_PARAMETER\": 0.99,\n",
    "    \"POP_SIZE\": 16,\n",
    "    \"ES_SEED\": 7,\n",
    "    \"RC_SEED\": 23,\n",
    "    \"NUM_GENERATIONS\":1,\n",
    "    # \"INT_LAMBDA\": 0.001,\n",
    "}\n",
    "\n",
    "environments = [\n",
    "    \"Asterix-MinAtar\",\n",
    "    # \"Breakout-MinAtar\",\n",
    "    # \"Freeway-MinAtar\",\n",
    "    # \"SpaceInvaders-MinAtar\",\n",
    "]\n",
    "\n",
    "\n",
    "def make_config_env(config, env_name):\n",
    "    config[\"ENV_NAME\"] = env_name\n",
    "    num_devices = jax.local_device_count()\n",
    "    assert config[\"NUM_ENVS\"] % num_devices == 0\n",
    "    config[\"NUM_ENVS_PER_DEVICE\"] = config[\"NUM_ENVS\"] // 1\n",
    "    config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] = config[\"TOTAL_TIMESTEPS\"] // 1\n",
    "    # config[\"EVAL_EPISODES_PER_DEVICE\"] = config[\"EVAL_EPISODES\"] // num_devices\n",
    "    config[\"NUM_UPDATES\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] // config[\"NUM_STEPS\"] // config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "    )\n",
    "    config[\"MINIBATCH_SIZE\"] = (\n",
    "        config[\"NUM_ENVS_PER_DEVICE\"] * config[\"NUM_STEPS\"] // config[\"NUM_MINIBATCHES\"]\n",
    "    )\n",
    "    config[\"TRAINING_HORIZON\"] = (\n",
    "        config[\"TOTAL_TIMESTEPS_PER_DEVICE\"] // config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "    )\n",
    "    env, env_params = gymnax.make(config[\"ENV_NAME\"])\n",
    "    env = FlattenObservationWrapper(env)\n",
    "    env = LogWrapper(env)\n",
    "    env = VecEnv(env)\n",
    "\n",
    "    return config, env, env_params\n",
    "\n",
    "\n",
    "def ppo_make_train(rng):\n",
    "    def linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"LR\"] * frac\n",
    "\n",
    "    def pred_linear_schedule(count):\n",
    "        frac = (\n",
    "            1.0\n",
    "            - (count // (config[\"NUM_MINIBATCHES\"] * config[\"UPDATE_EPOCHS\"]))\n",
    "            / config[\"NUM_UPDATES\"]\n",
    "        )\n",
    "        return config[\"PRED_LR\"] * frac\n",
    "\n",
    "    # INIT NETWORK\n",
    "    network = PPOActorCritic(env.action_space(env_params).n, activation=config[\"ACTIVATION\"])\n",
    "    target = BYOLTarget(128)\n",
    "    pred = AtariBYOLPredictor(128, env.action_space(env_params).n)\n",
    "\n",
    "    # KEYS\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    rng, _tar_rng = jax.random.split(rng)\n",
    "    # rng, _en_rng = jax.random.split(rng)\n",
    "    rng, _pred_rng = jax.random.split(rng)\n",
    "\n",
    "    # INIT INPUT\n",
    "    init_x = jnp.zeros((1, config[\"NUM_ENVS_PER_DEVICE\"], *env.observation_space(env_params).shape))\n",
    "    init_action = jnp.zeros((config[\"NUM_ENVS_PER_DEVICE\"],), dtype=jnp.int32)\n",
    "    close_init_hstate = CloseScannedRNN.initialize_carry(config[\"NUM_ENVS_PER_DEVICE\"], 128)\n",
    "    open_init_hstate = OpenScannedRNN.initialize_carry(config[\"NUM_ENVS_PER_DEVICE\"], 128)\n",
    "    init_bt = jnp.zeros((1, config[\"NUM_ENVS_PER_DEVICE\"], 128))\n",
    "\n",
    "    init_pred_input = (init_bt, init_x, init_action[np.newaxis, :])\n",
    "\n",
    "    network_params = network.init(_rng, init_x)\n",
    "    pred_params = pred.init(_pred_rng, close_init_hstate, open_init_hstate, init_pred_input)\n",
    "    target_params = target.init(_tar_rng, init_x)\n",
    "\n",
    "    if config[\"ANNEAL_LR\"]:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(learning_rate=linear_schedule, eps=1e-5),\n",
    "        )\n",
    "    else:\n",
    "        tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(config[\"LR\"], eps=1e-5),\n",
    "        )\n",
    "\n",
    "    if config[\"ANNEAL_PRED_LR\"]:\n",
    "        pred_tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(learning_rate=pred_linear_schedule, eps=1e-5),\n",
    "        )\n",
    "    else:\n",
    "        pred_tx = optax.chain(\n",
    "            optax.clip_by_global_norm(config[\"MAX_GRAD_NORM\"]),\n",
    "            optax.adam(config[\"PRED_LR\"], eps=1e-5),\n",
    "        )\n",
    "\n",
    "    train_state = TrainState.create(\n",
    "        apply_fn=network.apply,\n",
    "        params=network_params,\n",
    "        tx=tx,\n",
    "    )\n",
    "    pred_state = TrainState.create(\n",
    "        apply_fn=pred.apply,\n",
    "        params=pred_params,\n",
    "        tx=pred_tx,\n",
    "    )\n",
    "\n",
    "    target_state = TrainState.create(\n",
    "        apply_fn=target.apply,\n",
    "        params=target_params,\n",
    "        tx=pred_tx,\n",
    "    )\n",
    "\n",
    "    rng = jax.random.split(rng, jax.local_device_count())\n",
    "\n",
    "    return (\n",
    "        rng,\n",
    "        train_state,\n",
    "        pred_state,\n",
    "        target_state,\n",
    "        init_bt,\n",
    "        close_init_hstate,\n",
    "        open_init_hstate,\n",
    "        init_action,\n",
    "    )\n",
    "\n",
    "\n",
    "def train(\n",
    "    rng,\n",
    "    rc_params,\n",
    "    train_state,\n",
    "    pred_state,\n",
    "    target_state,\n",
    "    init_bt,\n",
    "    close_init_hstate,\n",
    "    open_init_hstate,\n",
    "    init_action,\n",
    "):\n",
    "    # REWARD COMBINER\n",
    "    rc_network=TemporalRewardCombiner()\n",
    "    # INIT STUFF FOR OPTIMIZATION AND NORMALIZATION\n",
    "    update_target_counter = 0\n",
    "    byol_reward_norm_params = BYOLRewardNorm(0, 0, 1, 0)\n",
    "\n",
    "    # INIT ENV\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "    obsv, env_state = env.reset(reset_rng, env_params)\n",
    "\n",
    "    # TRAIN LOOP\n",
    "    def _update_step(runner_state, unused):\n",
    "        # COLLECT TRAJECTORIES\n",
    "        def _env_step(runner_state, unused):\n",
    "            (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                bt,\n",
    "                close_hstate,\n",
    "                open_hstate,\n",
    "                last_act,\n",
    "                env_state,\n",
    "                last_obs,\n",
    "                byol_reward_norm_params,\n",
    "                update_target_counter,\n",
    "                rng,\n",
    "            ) = runner_state\n",
    "\n",
    "            # SELECT ACTION\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            pi, value = train_state.apply_fn(train_state.params, last_obs[np.newaxis, :])\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # STEP ENV\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            rng_step = jax.random.split(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "            obsv, env_state, reward, done, info = env.step(\n",
    "                rng_step, env_state, action.squeeze(0), env_params\n",
    "            )\n",
    "\n",
    "            # TIME STEP\n",
    "\n",
    "            norm_time_step = info[\"timestep\"]/config[\"TRAINING_HORIZON\"]\n",
    "\n",
    "            # INT REWARD\n",
    "            tar_obs = target_state.apply_fn(target_state.params, obsv[np.newaxis, :])\n",
    "            pred_input = (bt, last_obs[np.newaxis, :], last_act[np.newaxis, :])\n",
    "            pred_obs, new_bt, new_close_hstate, new_open_hstate = pred_state.apply_fn(\n",
    "                pred_state.params, close_hstate, open_hstate, pred_input\n",
    "            )\n",
    "            pred_norm = (pred_obs.squeeze(0)) / (\n",
    "                jnp.linalg.norm(pred_obs.squeeze(0), axis=-1, keepdims=True)\n",
    "            )\n",
    "            tar_norm = jax.lax.stop_gradient(\n",
    "                (tar_obs.squeeze(0)) / (jnp.linalg.norm(tar_obs.squeeze(0), axis=-1, keepdims=True))\n",
    "            )\n",
    "            int_reward = jnp.square(jnp.linalg.norm((pred_norm - tar_norm), axis=-1)) * (1 - done)\n",
    "            value, action, log_prob = (value.squeeze(0), action.squeeze(0), log_prob.squeeze(0))\n",
    "            transition = Transition(\n",
    "                done,\n",
    "                last_act,\n",
    "                action,\n",
    "                value,\n",
    "                reward,\n",
    "                int_reward,\n",
    "                log_prob,\n",
    "                last_obs,\n",
    "                obsv,\n",
    "                bt,\n",
    "                norm_time_step,\n",
    "                info,\n",
    "            )\n",
    "            runner_state = (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                new_bt,\n",
    "                new_close_hstate,\n",
    "                new_open_hstate,\n",
    "                action,\n",
    "                env_state,\n",
    "                obsv,\n",
    "                byol_reward_norm_params,\n",
    "                update_target_counter,\n",
    "                rng,\n",
    "            )\n",
    "            return runner_state, transition\n",
    "\n",
    "        close_initial_hstate, open_initial_hstate = runner_state[4:6]\n",
    "        runner_state, traj_batch = jax.lax.scan(_env_step, runner_state, None, config[\"NUM_STEPS\"])\n",
    "\n",
    "        # CALCULATE ADVANTAGE\n",
    "        (\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            target_state,\n",
    "            bt,\n",
    "            close_hstate,\n",
    "            open_hstate,\n",
    "            last_act,\n",
    "            env_state,\n",
    "            last_obs,\n",
    "            byol_reward_norm_params,\n",
    "            update_target_counter,\n",
    "            rng,\n",
    "        ) = runner_state\n",
    "\n",
    "        # update_target_counter+=1\n",
    "        _, last_val = train_state.apply_fn(train_state.params, last_obs[np.newaxis, :])\n",
    "\n",
    "        def _calculate_gae(traj_batch, last_val, byol_reward_norm_params):\n",
    "            norm_int_reward, byol_reward_norm_params = byol_normalize_prior_int_rewards(\n",
    "                traj_batch.int_reward, byol_reward_norm_params, config[\"REW_NORM_PARAMETER\"]\n",
    "            )\n",
    "            norm_traj_batch = Transition(\n",
    "                traj_batch.done,\n",
    "                traj_batch.prev_action,\n",
    "                traj_batch.action,\n",
    "                traj_batch.value,\n",
    "                traj_batch.reward,\n",
    "                norm_int_reward,\n",
    "                traj_batch.log_prob,\n",
    "                traj_batch.obs,\n",
    "                traj_batch.next_obs,\n",
    "                traj_batch.bt,\n",
    "                traj_batch.norm_time_step,\n",
    "                traj_batch.info,\n",
    "            )\n",
    "\n",
    "            def _get_advantages(gae_and_next_value, transition):\n",
    "                gae, next_value = gae_and_next_value\n",
    "                done, value, reward, int_reward, norm_time_step = (\n",
    "                    transition.done,\n",
    "                    transition.value,\n",
    "                    transition.reward,\n",
    "                    transition.int_reward,\n",
    "                    transition.norm_time_step,\n",
    "                )\n",
    "                rc_input = jnp.concatenate(\n",
    "                    (reward[:, None], int_reward[:, None], norm_time_step[:, None]), axis=-1\n",
    "                )              \n",
    "                int_lambda = rc_network.apply(rc_params,rc_input)\n",
    "                delta = (\n",
    "                    (reward + (int_reward * int_lambda))\n",
    "                    + config[\"GAMMA\"] * next_value * (1 - done)\n",
    "                    - value\n",
    "                )\n",
    "                gae = delta + config[\"GAMMA\"] * config[\"GAE_LAMBDA\"] * (1 - done) * gae\n",
    "                return (gae, value), gae\n",
    "\n",
    "            _, advantages = jax.lax.scan(\n",
    "                _get_advantages,\n",
    "                (jnp.zeros_like(last_val), last_val),\n",
    "                norm_traj_batch,\n",
    "                reverse=True,\n",
    "                unroll=16,\n",
    "            )\n",
    "            return (\n",
    "                advantages,\n",
    "                advantages + traj_batch.value,\n",
    "                norm_int_reward,\n",
    "                byol_reward_norm_params,\n",
    "            )\n",
    "\n",
    "        advantages, targets, norm_int_reward, byol_reward_norm_params = _calculate_gae(\n",
    "            traj_batch, last_val.squeeze(0), byol_reward_norm_params\n",
    "        )\n",
    "\n",
    "        # UPDATE NETWORK\n",
    "        def _update_epoch(update_state, unused):\n",
    "            def _update_minbatch(train_states, batch_info):\n",
    "                traj_batch, advantages, targets, init_close_hstate, init_open_hstate = batch_info\n",
    "                train_state, pred_state, target_state, update_target_counter = train_states\n",
    "\n",
    "                def pred_loss(\n",
    "                    pred_params, target_params, traj_batch, init_close_hstate, init_open_hstate\n",
    "                ):\n",
    "                    tar_obs = target_state.apply_fn(target_params, traj_batch.next_obs)\n",
    "                    pred_input = (traj_batch.bt, traj_batch.obs, traj_batch.prev_action)\n",
    "                    pred_obs, _, _, _ = pred_state.apply_fn(\n",
    "                        pred_params, init_close_hstate[0], init_open_hstate[0], pred_input\n",
    "                    )\n",
    "                    pred_norm = (pred_obs) / (jnp.linalg.norm(pred_obs, axis=-1, keepdims=True))\n",
    "                    tar_norm = jax.lax.stop_gradient(\n",
    "                        (tar_obs) / (jnp.linalg.norm(tar_obs, axis=-1, keepdims=True))\n",
    "                    )\n",
    "                    loss = jnp.square(jnp.linalg.norm((pred_norm - tar_norm), axis=-1)) * (\n",
    "                        1 - traj_batch.done\n",
    "                    )\n",
    "                    return loss.mean()\n",
    "\n",
    "                def _loss_fn(params, traj_batch, gae, targets):\n",
    "                    # RERUN NETWORK\n",
    "                    pi, value = train_state.apply_fn(params, traj_batch.obs)\n",
    "                    log_prob = pi.log_prob(traj_batch.action)\n",
    "\n",
    "                    # CALCULATE VALUE LOSS\n",
    "                    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
    "                        -config[\"CLIP_EPS\"], config[\"CLIP_EPS\"]\n",
    "                    )\n",
    "                    value_losses = jnp.square(value - targets)\n",
    "                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
    "\n",
    "                    # CALCULATE ACTOR LOSS\n",
    "                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = (\n",
    "                        jnp.clip(\n",
    "                            ratio,\n",
    "                            1.0 - config[\"CLIP_EPS\"],\n",
    "                            1.0 + config[\"CLIP_EPS\"],\n",
    "                        )\n",
    "                        * gae\n",
    "                    )\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
    "                    loss_actor = loss_actor.mean()\n",
    "                    entropy = pi.entropy().mean()\n",
    "\n",
    "                    total_loss = (\n",
    "                        loss_actor + config[\"VF_COEF\"] * value_loss - config[\"ENT_COEF\"] * entropy\n",
    "                    )\n",
    "                    return total_loss, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                (loss, (vloss, aloss, entropy)), grads = jax.value_and_grad(_loss_fn, has_aux=True)(\n",
    "                    train_state.params, traj_batch, advantages, targets\n",
    "                )\n",
    "                pred_losses, pred_grads = jax.value_and_grad(pred_loss)(\n",
    "                    pred_state.params,\n",
    "                    target_state.params,\n",
    "                    traj_batch,\n",
    "                    init_close_hstate,\n",
    "                    init_open_hstate,\n",
    "                )\n",
    "                # (loss, vloss, aloss, entropy, pred_losses, grads, pred_grads) = jax.lax.pmean(\n",
    "                #     (loss, vloss, aloss, entropy, pred_losses, grads, pred_grads),\n",
    "                #     axis_name=\"devices\",\n",
    "                # )\n",
    "\n",
    "                def update_target(\n",
    "                    target_state, pred_state, update_target_counter=update_target_counter\n",
    "                ):\n",
    "                    def true_fun(_):\n",
    "                        # Perform the EMA update\n",
    "                        return update_target_state_with_ema(\n",
    "                            predictor_state=pred_state,\n",
    "                            target_state=target_state,\n",
    "                            ema_param=config[\"EMA_PARAMETER\"],\n",
    "                        )\n",
    "\n",
    "                    def false_fun(_):\n",
    "                        # Return the old target_params unchanged\n",
    "                        return target_state\n",
    "\n",
    "                    # Conditionally update every 10 steps\n",
    "                    return jax.lax.cond(\n",
    "                        update_target_counter % 320 == 0,\n",
    "                        true_fun,\n",
    "                        false_fun,\n",
    "                        None,  # The argument passed to true_fun and false_fun, `_` in this case is unused\n",
    "                    )\n",
    "\n",
    "                update_target_counter += 1\n",
    "                train_state = train_state.apply_gradients(grads=grads)\n",
    "                pred_state = pred_state.apply_gradients(grads=pred_grads)\n",
    "                target_state = update_target(target_state, pred_state, update_target_counter)\n",
    "\n",
    "                return (train_state, pred_state, target_state, update_target_counter), (\n",
    "                    loss,\n",
    "                    (vloss, aloss, entropy),\n",
    "                    pred_losses,\n",
    "                )\n",
    "\n",
    "            (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                update_target_counter,\n",
    "                init_close_hstate,\n",
    "                init_open_hstate,\n",
    "                traj_batch,\n",
    "                advantages,\n",
    "                targets,\n",
    "                rng,\n",
    "            ) = update_state\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            permutation = jax.random.permutation(_rng, config[\"NUM_ENVS_PER_DEVICE\"])\n",
    "            batch = (traj_batch, advantages, targets, init_close_hstate, init_open_hstate)\n",
    "\n",
    "            shuffled_batch = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.take(x, permutation, axis=1), batch\n",
    "            )\n",
    "            minibatches = jax.tree_util.tree_map(\n",
    "                lambda x: jnp.swapaxes(\n",
    "                    jnp.reshape(\n",
    "                        x,\n",
    "                        [x.shape[0], config[\"NUM_MINIBATCHES\"], -1] + list(x.shape[2:]),\n",
    "                    ),\n",
    "                    1,\n",
    "                    0,\n",
    "                ),\n",
    "                shuffled_batch,\n",
    "            )\n",
    "            (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                update_target_counter,\n",
    "            ), total_loss = jax.lax.scan(\n",
    "                _update_minbatch,\n",
    "                (train_state, pred_state, target_state, update_target_counter),\n",
    "                minibatches,\n",
    "            )\n",
    "            update_state = (\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                update_target_counter,\n",
    "                init_close_hstate,\n",
    "                init_open_hstate,\n",
    "                traj_batch,\n",
    "                advantages,\n",
    "                targets,\n",
    "                rng,\n",
    "            )\n",
    "            return update_state, total_loss\n",
    "\n",
    "        traj_batch = Transition(\n",
    "            traj_batch.done,\n",
    "            traj_batch.prev_action,\n",
    "            traj_batch.action,\n",
    "            traj_batch.value,\n",
    "            traj_batch.reward,\n",
    "            traj_batch.int_reward,\n",
    "            traj_batch.log_prob,\n",
    "            traj_batch.obs,\n",
    "            traj_batch.next_obs,\n",
    "            traj_batch.bt.squeeze(1),\n",
    "            traj_batch.norm_time_step,\n",
    "            traj_batch.info,\n",
    "        )\n",
    "\n",
    "        update_state = (\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            target_state,\n",
    "            update_target_counter,\n",
    "            open_initial_hstate[np.newaxis, :],\n",
    "            close_initial_hstate[np.newaxis, :],\n",
    "            traj_batch,\n",
    "            advantages,\n",
    "            targets,\n",
    "            rng,\n",
    "        )\n",
    "        update_state, loss_info = jax.lax.scan(\n",
    "            _update_epoch, update_state, None, config[\"UPDATE_EPOCHS\"]\n",
    "        )\n",
    "        train_state, pred_state, target_state, update_target_counter = update_state[:4]\n",
    "        metric = traj_batch.info\n",
    "        rng = update_state[-1]\n",
    "        if config.get(\"DEBUG\"):\n",
    "\n",
    "            def callback(info):\n",
    "                return_values = info[\"returned_episode_returns\"][info[\"returned_episode\"]]\n",
    "                timesteps = (\n",
    "                    info[\"timestep\"][info[\"returned_episode\"]] * config[\"NUM_ENVS_PER_DEVICE\"]\n",
    "                )\n",
    "                for t in range(len(timesteps)):\n",
    "                    print(f\"global step={timesteps[t]}, episodic return={return_values[t]}\")\n",
    "\n",
    "            jax.debug.callback(callback, metric)\n",
    "\n",
    "        runner_state = (\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            target_state,\n",
    "            bt,\n",
    "            close_hstate,\n",
    "            open_hstate,\n",
    "            last_act,\n",
    "            env_state,\n",
    "            last_obs,\n",
    "            byol_reward_norm_params,\n",
    "            update_target_counter,\n",
    "            rng,\n",
    "        )\n",
    "        return runner_state, (metric, loss_info, norm_int_reward, traj_batch.int_reward)\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (\n",
    "        train_state,\n",
    "        pred_state,\n",
    "        target_state,\n",
    "        init_bt,\n",
    "        close_init_hstate,\n",
    "        open_init_hstate,\n",
    "        init_action,\n",
    "        env_state,\n",
    "        obsv,\n",
    "        byol_reward_norm_params,\n",
    "        update_target_counter,\n",
    "        _rng,\n",
    "    )\n",
    "    runner_state, extra_info = jax.lax.scan(_update_step, runner_state, None, config[\"NUM_UPDATES\"])\n",
    "    metric, rl_total_loss, int_reward, norm_int_reward = extra_info\n",
    "    return {\n",
    "        \"train_state\": runner_state[0],\n",
    "        \"metrics\": metric,\n",
    "        \"rl_total_loss\": rl_total_loss[0],\n",
    "        \"rl_value_loss\": rl_total_loss[1][0],\n",
    "        \"rl_actor_loss\": rl_total_loss[1][1],\n",
    "        \"rl_entrophy_loss\": rl_total_loss[1][2],\n",
    "        \"pred_loss\": rl_total_loss[2],\n",
    "        \"int_reward\": int_reward,\n",
    "        \"norm_int_reward\": norm_int_reward,\n",
    "        \"rng\": runner_state[-1]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for env_name in environments:\n",
    "    rng = jax.random.PRNGKey(config[\"SEED\"])\n",
    "    config, env, env_params = make_config_env(config, env_name)\n",
    "    print(f\"Training in {config['ENV_NAME']}\")\n",
    "\n",
    "    make_train=jax.jit(jax.vmap(ppo_make_train, out_axes=(1, 0, 0, 0, 0, 0, 0, 0)))\n",
    "    train_fn = jax.vmap(train, in_axes=(0, None,0,0,0,0,0,0,0))\n",
    "    train_fn = jax.vmap(train_fn, in_axes=(None, 0,None,None,None,None,None,None,None))\n",
    "    train_fn = jax.pmap(train_fn, axis_name=\"devices\")\n",
    "\n",
    "    group=\"reward_combiners\"\n",
    "    tags=[\"meta-learner\", config[\"ENV_NAME\"]]\n",
    "    name=f'{config[\"RUN_NAME\"]}_{config[\"ENV_NAME\"]}'\n",
    "    # fit_log=wandb.init(\n",
    "    #             project=\"MetaLearnCuriosity\",\n",
    "    #             config=config,\n",
    "    #             group=group,\n",
    "    #             tags=tags,\n",
    "    #             name=f\"{name}_fitness\",\n",
    "    #         )\n",
    "    reward_combiner_network = TemporalRewardCombiner()\n",
    "    rc_params_pholder = reward_combiner_network.init(\n",
    "        jax.random.PRNGKey(config[\"RC_SEED\"]), jnp.zeros((1, 3))\n",
    "    )\n",
    "    es_rng = jax.random.PRNGKey(config[\"ES_SEED\"])\n",
    "    strategy = OpenES(\n",
    "        popsize=config[\"POP_SIZE\"],\n",
    "        pholder_params=rc_params_pholder,\n",
    "        opt_name=\"adam\",\n",
    "        lrate_init=2e-4,\n",
    "    )\n",
    "\n",
    "    es_rng, es_rng_init = jax.random.split(es_rng)\n",
    "    es_params = strategy.default_params\n",
    "    es_state = strategy.initialize(es_rng_init, es_params)\n",
    "\n",
    "    for _ in tqdm(range(config[\"NUM_GENERATIONS\"]), desc=\"Processing Generations\"):\n",
    "        rng, rng_seeds = jax.random.split(rng)\n",
    "        rng_train = jax.random.split(rng_seeds, config[\"NUM_SEEDS\"])\n",
    "\n",
    "        # setting up the RL agents.\n",
    "        (\n",
    "            rng_train,\n",
    "            train_state,\n",
    "            pred_state,\n",
    "            target_state,\n",
    "            init_bt,\n",
    "            close_init_hstate,\n",
    "            open_init_hstate,\n",
    "            init_action,\n",
    "        ) = make_train(rng_train)\n",
    "\n",
    "        # duplicating here for pmap\n",
    "        open_init_hstate = replicate(open_init_hstate, jax.local_devices())\n",
    "        close_init_hstate = replicate(close_init_hstate, jax.local_devices())\n",
    "        train_state = replicate(train_state, jax.local_devices())\n",
    "        pred_state = replicate(pred_state, jax.local_devices())\n",
    "        target_state = replicate(target_state, jax.local_devices())\n",
    "        init_bt = replicate(init_bt, jax.local_devices())\n",
    "        init_action = replicate(init_action, jax.local_devices())\n",
    "        t = time.time()\n",
    "\n",
    "        # Fitness evaulation\n",
    "        es_rng, es_rng_ask = jax.random.split(es_rng)\n",
    "        x, es_state = strategy.ask(es_rng_ask, es_state, es_params)\n",
    "        output = jax.block_until_ready(\n",
    "            train_fn(\n",
    "                rng_train,\n",
    "                x,\n",
    "                train_state,\n",
    "                pred_state,\n",
    "                target_state,\n",
    "                init_bt,\n",
    "                close_init_hstate,\n",
    "                open_init_hstate,\n",
    "                init_action,\n",
    "            )\n",
    "        )\n",
    "        rewards = output[\"metrics\"][\"sum_of_rewards\"]\n",
    "        print(rewards.shape)\n",
    "        fitness = rewards.mean(-1).mean(2).reshape(rewards.shape[0], rewards.shape[1], -1).sum(-1)\n",
    "        es_state = strategy.tell(x, fitness, es_state, es_params)\n",
    "        elapsed_time = time.time() - t\n",
    "        print(f\"Done in {elapsed_time / 60:.2f}min\")\n",
    "    #     fit_log.log({f\"{name}_mean_fitness\":fitness.mean(),\n",
    "    #                  f\"{name}_best_fitness\":jnp.max(fitness)})\n",
    "\n",
    "    # fit_log.finish()\n",
    "    # logger = WBLogger(\n",
    "    #     config=config,\n",
    "    #     group=group,\n",
    "    #     tags=tags,\n",
    "    #     name=name,\n",
    "    # )\n",
    "    # # Get the absolute path of the directory\n",
    "    # checkpoint_directory = f'MLC_logs/flax_ckpt/{config[\"ENV_NAME\"]}/{config[\"RUN_NAME\"]}'\n",
    "    params=strategy.param_reshaper.reshape_single(es_state.mean[0])\n",
    "    # path = os.path.abspath(checkpoint_directory)\n",
    "    # Save(path, params)\n",
    "    # logger.save_artifact(path)\n",
    "    # shutil.rmtree(path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpu_curiosax",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
